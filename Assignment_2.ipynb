{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Modelling Assignment 2  <font color='red'>Deadline = 25 april 23:59</font>\n",
    "\n",
    "In totaal kan je voor deze opdracht __81 punten__ halen + 5 bonuspunten.\n",
    "\n",
    "## Fitting functions\n",
    "\n",
    "Hieronder volgen wat korte vragen en uitleg over model fitting ter voorbereiding op de verdere assignments. Een deel van de theorie komt misschien bekend voor. Het is de bedoeling dat je op basis van deze simpele opdracht inziet hoe het fitten van een cognitief model niet veel verschilt van het fitten van een wiskundige formule op data. In de basis is het idee dat je door een bepaalde methode de parameters van de desbetreffende functie kan vinden die de data, of het gedrag van mensen, zo goed mogelijk kan beschrijven. \n",
    "\n",
    "Wegens historische redenen is het eerste deel van deze opdracht nog in het Engels, je mag in het Nederlands of in het Engels antwoorden. \n",
    "\n",
    "Bij elke vraag staat de hoeveelheid punten die je er voor kan krijgen. Geef antwoorden in blokken met code of met tekst. Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hierom wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig. \n",
    "\n",
    "Sla het uiteindelijke notebook __met al gerunde output__ op, met jullie studentnummers en achternamen in de filenaam: `studentnummer1_achternaam1_studentnummer2_achternaam2_opdrachtnummer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Fitting functions\n",
    "\n",
    "Voor deze inleidende opdracht schrijven we verschillende functies en gaan deze op data \"fitten\". De meeste concepten hier moeten bekend zijn uit de cursus *Leren*, behalve dat we de *SciPy*-library zullen gebruiken om de functies te fitten op data. Met fitten bedoelen we de optimale waarden vinden van de parameters in de functie zodat deze de data het best mogelijk benadert. De *SciPy*-library komt goed van pas bij latere opdrachten, dus het is handig om hier alvast wat elementen te introduceren.\n",
    "\n",
    "### Generating simulation data\n",
    "\n",
    "Assume there is some model that is defined by the following function:\n",
    "\n",
    "$$y = e^{\\frac{3 x}{20}}$$\n",
    "\n",
    "Imagine that this represents the relationship between the temperature (`x`) and the amount of ice cream sold (`y`) in a small ice cream store. The higher the temperature, the more ice cream is sold. For an ice cream shop owner it would be good to know this exact relationship, such that she can predict how much to have in store when the temperature is changing. \n",
    "\n",
    "Of course, such a relationship cannot be found in a book but must be estimated from data. So in the first step we will generate some data. Below, we:  \n",
    "\n",
    "* Write a function `curve` which implements this model\n",
    "* Generate 50 uniformly spaced x-values over the interval `[1, 13]` using [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n",
    "* Apply the curve function to the entire *ndarray* of x-values using [vectorize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html)\n",
    "\n",
    "The shop owner is taking note of the ice cream sold on every day but of course such measurements are noisy: not on each day that it is 10 degrees celcius will the exact same amount of people show up. Their ice cream buying behavior will be determined not only by the temperature but also many other factors (e.g. how much money the customers have in their account) that are unknown to the shop owner. Therefore we will add some noise to the data. Below, we:\n",
    "\n",
    "* Create an *ndarray* of noise from a Gaussian distribution with $\\mu = 0.0$ and $\\sigma = 1.0$ using [random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html)\n",
    "* Add the noise to the curve results to create the artificial simulation data\n",
    "* Plot the actual underlying curve as a (curved) line and the simulated data as dots, both in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed=99999)\n",
    "\n",
    "def curve(x):\n",
    "    return math.e**(3*x/20)\n",
    "\n",
    "samples = np.linspace(1, 13)\n",
    "model_data = np.vectorize(curve)(samples)\n",
    "\n",
    "noise = np.random.normal(size=samples.shape)\n",
    "sim_data = model_data + noise\n",
    "\n",
    "plt.plot(samples, model_data, label='Actual curve')\n",
    "plt.plot(samples, sim_data, 'o', label='Measured data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q.0a Fitting a linear model (2 points)\n",
    "\n",
    "Now we'll use *SciPy* to fit a line through this simulation data. In *Leren* we needed to compute the partial derivative of the error function with respect to each of the model parameters to know how to modify those parameters to get the derivates (close to) zero. Here, we will just define what the error function is, and let *SciPy* do the actual minimization work. Let's assume that we are looking at the relationship between the temperature and ice cream sales volume. We expect that the higher the temperature, the more ice cream will be sold. \n",
    "\n",
    "The first model will be a very basic linear one, with only 1 parameter `a`. This model assumes there is a linear relationship between variables x and y:\n",
    "\n",
    "$$y = ax$$\n",
    "\n",
    "*(note: this line will always pass through the origin, in other words the intercept is zero)*\n",
    "\n",
    "* Write a function `linear` which implements this model\n",
    "\n",
    "It is our goal to find the optimal value for `a`.\n",
    "\n",
    "* Write a function `MSE` which computes the *Mean Squared Error* of an array of model estimates `y_hat` and an array of observed data `y`\n",
    "\n",
    "Already provided is a general function called `MSE_fit_func` which takes 4 arguments:\n",
    "1. *params:* A set of model parameters for the model function\n",
    "2. *func:* The model function that is being applied\n",
    "3. *x:* An array of x values\n",
    "4. *y:* An array of y values\n",
    "\n",
    "This function applies the model function to the *x* values using the model parameters and computes the resulting *MSE* with the observed *y* values. Assuming the data (i.e. the *x* and *y* values) and the model function are fixed, the model parameter(s) can be varied and the function will return an *MSE* for each parameter setting.\n",
    "\n",
    "The *SciPy* module *optimize* has a lot of functions to solve exactly this type of problem, where you change some parameters in order to minimize some function. As the current linear model has only one variable, we'll use the function [minimize_scalar](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar).\n",
    "\n",
    "The function we want to minimize is the `MSE_fit_func` (i.e. the MSE of applying the model function to the data), and as the minimization method we'll use `Brent`. There are quite a few different minimization methods *SciPy* offers, but we won't worry about their differences for now and just use the simple `Brent` method. All minimization functions will try and minimize the returned value of the function by varying its **first argument**. The other function arguments will remain fixed and may be provided with optional argument `args`. We finish the call to `minimize_scalar` by filling in the correct values for `args` to fit the linear model to the simulation data generated in *Q0*.\n",
    "\n",
    "The variable `fit` will now be an [OptimizeResult](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult) object.\n",
    "\n",
    "* Print the variable `fit` and some of its attributes to inspect the results of the minimization\n",
    "* Plot the simulation data as dots and the fitted linear model as a line, both in the same plot\n",
    "* Print the value of `a` that minimizes the linear model MSE on your data (`fit.x`)\n",
    "* Print the value of the MSE of the best fitting model (`fit.fun`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def linear(x, a):\n",
    "    # TO DO\n",
    "    return\n",
    "\n",
    "def MSE(y_hat, y):\n",
    "    # TO DO\n",
    "    return\n",
    "\n",
    "def MSE_fit_func(params, func, x, y):\n",
    "    return MSE(func(x, params), y)\n",
    "\n",
    "fit = optimize.minimize_scalar(MSE_fit_func, method='Brent', args=(linear, samples, sim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Q0.b Fitting a polynomial model (2 points) \n",
    "\n",
    "Now, the linear model is a pretty good fit, but the shopkeeper is not satisfied. Being a math hobbyist, she plans to create a new model using a $4^{th}$-order polynomial function:\n",
    "\n",
    "$$y=b_0+b_1x+b_2x^2+b_3x^3+b_4x^4$$\n",
    "\n",
    "* Write a function `polynomial` which implements this model, with the argument *b* being an *ndarray* containing all model parameters $b_0 \\dots b_4$\n",
    "\n",
    "When minimizing multiple parameters, we'll need to provide starting values for `b`, from where the minimization function will start the search. For now, you should use `np.array([-5, 9, -4, 1, .01])` as the starting point and we'll come back to selecting sensible starting values for the parameters later.\n",
    "\n",
    "* Use the function [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) to fit the polynomial model to the generated data from *Q0* using the `Nelder-Mead` method\n",
    "* Plot the simulation data as dots and the fitted polynomial model as a (curved) line, both in the same plot\n",
    "* Print the value of `b` that minimizes the polynomial model MSE on your data\n",
    "* Print the value of the MSE of the best fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, b):\n",
    "    # TO DO\n",
    "    return\n",
    "\n",
    "fit2 = optimize.minimize(MSE_fit_func, np.array([-5, 9, -4, 1, .01]), method='Nelder-Mead',\n",
    "                        args=(polynomial, samples, sim_data))\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q0.c Comparing models (4 points) \n",
    "\n",
    "Now, the linear model is a pretty good fit, but did the shopkeeper improve her model by making it more complex?\n",
    "\n",
    "* Compare the MSE of both models and report which one has the best fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shopkeeper did all her work in the winter, preparing for the summer to come. Hence, her models were fitted only to data corresponding to temperatures between 1 and 13. But now it is spring and new data is coming in. The shopkeeper will now use her best model to predict sales. However, she was surprised to learn what her model predicted!\n",
    "\n",
    "* Report the prediction of the model for 18 degrees celcius (`x = 18`), how does that compare to the best day in winter (`x = 13`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shopkeeper is again collecting data for 50 days to get a better picture of what is going on. \n",
    "\n",
    "* Generate 50 uniformly spaced x-values over the interval `[13, 20]` using [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n",
    "* Apply the `curve` function (the exponential function introduced earlier) to the entire *ndarray* of x-values using [vectorize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html)\n",
    "* Create an *ndarray* of noise from a Gaussian distribution with $\\mu = 0.0$ and $\\sigma = 1.0$ using [random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html)\n",
    "* Add the noise to the curve results to create the artificial simulation data\n",
    "\n",
    "We assume that this newly added artificial data is actually real data observed by the shopkeeper. She now compares the predictions of her original two models with what really happened. \n",
    "* Plot the new data (with `x` ranging from 13 to 20) as dots and the earlier fitted linear and polynomial models as (curved) lines in the new plot. Make sure to add labels for clearer distinction.\n",
    "* Compute and print the `MSE` for the new data (with `x` ranging from 13 to 20), for both of the earlier fitted models. Report which one has the best fit. Try to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO print values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cognitive Model Fitting\n",
    "\n",
    "In cognitive science, different models can represent different theories of how the mind works. These theories are formalized in mathematical equations. The different models will make different predictions about the cognitive processes of people and ultimately of their behavior. To differentiate between these models, they are often fit to real behavioral (or brain) data, and their \"fit\" to the data is compared. It is common practice to further \"punish\" models in relation the number of free parameters that they have (to prevent **overfitting**, see debacle with the polynomial above). We will get back to model fitting in part III below. \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# I. Q-Learning\n",
    "\n",
    "Q-learning algorithmes zijn gebaseerd op de Temporal Difference learning modellen die we eerder hebben besproken. Q-learning leert direct de associatie tussen states, actions en outcomes. De robot krijgt nu dus niet alleen maar beloningen, maar kan ook uitzoeken welke handeling de beste is geveven de situatie.\n",
    "\n",
    "Details over Q-learning zijn terug te vinden in de college slides en het hoofdstuk van Gureckis & Love [**computational reinforcement learning**](http://bradlove.org/papers/GureckisLovePress.pdf) en voor meer verdieping in het online boek van [**Sutton & Barto**](http://incompleteideas.net/book/bookdraft2018jan1.pdf) en dan met name hoofdstuk 6.\n",
    "\n",
    "Het leren in deze opdracht speelt zich af in een simpel **Markov Decision Process** met de volgende structuur:\n",
    "\n",
    "![](images/bandit_arms.png)\n",
    "\n",
    "In deze taak moet de robot telkens uit één van de schatkisten iets pakken. Sommige schatkisten leveren meer geld op dan andere, maar de robot weet in het begin nog niks over de schatkisten, en verwacht er maar weinig van. In elke ronde wordt de uitkomst van een schatkist bepaald door een trekking van een waarde uit een normaalverdeling met een ander gemiddelde. Het is aan de robot om er achter te komen welke van de vier schatkisten het meeste oplevert. De vier normaalverdelingen behorende bij de vier schatkisten verschillen in hun gemiddelde maar niet in hun variantie (standaard deviatie). \n",
    "\n",
    "We gaan Q-learing gebruiken om te beschrijven hoe de robot leert om de beste keuze te maken. De update-regel voor de Q-waarde is als volgt:\n",
    "\n",
    "$$ Q(s_t,a_t)= Q(s_t,a_t) + αδ $$\n",
    "\n",
    "waarbij $s_t$ de state op tijdstip $t$ is, $a_t$ de actie op tijdstip $t$, en $\\delta$ de prediction error. De actie is hier dus het kiezen van één van de kisten (totaal 4 mogelijke acties).\n",
    "\n",
    "**Let op:** In dit simpele experiment is er maar één state, waarin de robot telkens terugkeert na het maken van een keuze. Dit heeft als gevolg dat bij het leren geen rekening gehouden hoeft te worden met de actie die in de volgende state gemaakt wordt. De standaard prediction-error:\n",
    "\n",
    "$$\\delta = r_{t+1} + \\gamma\\ max_a\\ Q(s_{t+1} , a) − Q(s_t , a_t)$$\n",
    "\n",
    "verandert daardoor nu in:\n",
    "$$\\delta = r_{t+1} − Q(s_t , a_t)$$\n",
    "\n",
    "Aan het begin van het experiment heeft de robot geen enkele kennis van de wereld en geen enkele verwachtingen voor het krijgen van beloningen. Voor elke schatkist geldt:\n",
    "\n",
    "$$Q(1)=Q(2)=Q(3)=Q(4)=0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.a (2 punten)\n",
    "\n",
    "Schrijf de Q-learning functie op die de nieuwe waarde Q uitrekent nadat de robot een schatkist heeft uitgekozen. Deze functie is hierboven al gegeven, maar we willen hem nu als one-liner. In deze functie komt $\\alpha$ voor. Wat is de rol van $\\alpha$ bij het leren? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.b (2 punten)\n",
    "\n",
    "Stel, de robot selecteert schatkist 1 en krijgt een beloning van 2 munten. Wat is hierna de waarde van $Q(1)$? Rapporteer dit voor\n",
    "$\\alpha=0.5$ en $\\alpha=0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c (8 punten)\n",
    "\n",
    "We gaan er nu van uit dat de robot de $\\epsilon$-greedy keuzeregel toepast. Dus bij het exploreren (kans $\\epsilon$) wordt er een random kist gekozen, en bij het exploiteren (kans $1-\\epsilon$) wordt de kist met de hoogste Q-waarde gekozen. Als meerdere kisten de hoogste Q-waarde hebben, dan wordt daar random één van gekozen.\n",
    "\n",
    "Schrijf een functie `q_learn` die $\\alpha$ (alpha), $\\epsilon$ (epsilon) en het aantal rondes (trials) als input accepteert. De output van deze functie moet onder andere een lijst `Q` bevatten met $Q$-waarden voor elke schatkist (1 t/m 4) in de wereld van de robot na het leren gedurende het gegeven aantal rondes. \n",
    "\n",
    "* Initieer de verwachtingen van de robot voor de 4 schatkisten:\n",
    "    * $Q(1) = Q(2) = Q(3) = Q(4) = 0$.\n",
    "* Initieer de lijst met alle verkregen beloningen: `outcomes = []`\n",
    "* Initieer de som van alle verkregen beloningen: `total_score = 0`\n",
    "* Initieer keuzelijsten:\n",
    "    * `choice_1 = np.zeros(trials)`\n",
    "    * `choice_2 = np.zeros(trials)`\n",
    "    * `choice_3 = np.zeros(trials)`\n",
    "    * `choice_4 = np.zeros(trials)`\n",
    "* Initieer de parameters van de normaalverdelingen waaruit de beloningen van de 4 schatkisten telkens worden getrokken:\n",
    "    * Kist 1: mean=20, SD=4\n",
    "    * Kist 2: mean=30, SD=4\n",
    "    * Kist 3: mean=50, SD=4\n",
    "    * Kist 4: mean=70, SD=4\n",
    "* Creëer een for-loop over alle rondes:\n",
    "    * Elke ronde selecteert de robot een kist op basis van $\\epsilon$-greedy. __Let op:__ e-greedy kan afwijken van de formules in het college. Dit is correct:\n",
    "    ```python\n",
    "            if random.random() < epsilon:\n",
    "                # explore\n",
    "            else:\n",
    "                # exploit\n",
    "    ```\n",
    "    * Als de robot in de ronde met index `i` (zero-based indexing) kiest voor schatkist `j`, dan update je `choice_j[i] = 1`.\n",
    "    * Kijk wat de beloning is na het maken van een keuze, en update de Q-waarde van de gekozen kist. Append de verkregen beloning aan de lijst `outcomes`, en tel de verkregen beloning op bij `total_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(alpha, epsilon, trials=200):\n",
    "    # TO DO\n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laat met behulp van deze functie de robot 200 rondes leren over deze wereld (1 leer-episode bestaat dus uit 200 rondes). Wat zijn de verwachtingen (Q-waarden) voor de schatkisten aan het eind van het experiment? En wat is de totale score? Geef voor:\n",
    "\n",
    "1. $\\alpha = 0.1$ en $\\epsilon = 0.1$\n",
    "2. $\\alpha = 0.5$ en $\\epsilon = 0.1$\n",
    "\n",
    "__Let op:__ We hebben te maken met toevalsexperimenten, dus iedere keer dat je je code runt zal de output anders zijn. Run je code daarom meerdere keren (voor beide waarden van $\\alpha$), bekijk telkens de output, en rapporteer vervolgens de output (Q-waarden en totale score) van een leer-episode die jij representatief vindt voor alles wat je langs hebt zien komen. Tip: na het maken van Q1.d zal je een beter beeld hebben van wat representatief is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laat nu voor beide modellen zien hoe de keuzes voor de verschillende kisten veranderen gedurende de trials door middel van 2 plots (model 1: $\\alpha = 0.1$ en $\\epsilon = 0.1$) (model 2: $\\alpha = 0.5$ en $\\epsilon = 0.1$). Voor het plotten van de keuzes is het handig om naar de keuzeratio's (op een schaal van 0 tot 1) per 10 trials te kijken. Bijvoorbeeld: een keuzeratio van 1.0 voor schatkist 4 voor de eerste 10 trials wil zeggen dat schatkist 4 gedurende die 10 trials elke keer is gekozen door de robot. Deze code kun je gebruiken voor je plot behorende bij model 2 (kopieer en pas zelf aan voor de plot behorende bij model 1):\n",
    "```python\n",
    "res_05_01 = q_learn(0.5, 0.1)   # roep functie aan voor model 2\n",
    "width = 10                      # de grootte van de bin\n",
    "\n",
    "# bereken de keuzeratio's voor alle schatkisten, voor alle bins\n",
    "result1 = res_05_01[2].reshape(-1, width).mean(axis=1)\n",
    "result2 = res_05_01[3].reshape(-1, width).mean(axis=1)\n",
    "result3 = res_05_01[4].reshape(-1, width).mean(axis=1)\n",
    "result4 = res_05_01[5].reshape(-1, width).mean(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.plot(result1, label=r\"$1$\")\n",
    "plt.plot(result2, label=r\"$2$\")\n",
    "plt.plot(result3, label=r\"$3$\")\n",
    "plt.plot(result4, label=r\"$4$\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"choice ratio\")\n",
    "plt.xlabel(\"trials * 10\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "__Let op:__ We hebben te maken met toevalsexperimenten, dus iedere keer dat je je code runt zullen de plots anders zijn. Run je code daarom meerdere keren (voor beide modellen), bekijk telkens de resulterende plots, en voeg vervolgens plots in die jij representatief vindt voor alles wat je langs hebt zien komen. Tip: na het maken van Q1.d zal je een beter beeld hebben van wat representatief is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terugblikkend op je vorige twee antwoorden:\n",
    "* Welk van de twee modellen zit doorgaans dichter bij de waarheid als je kijkt naar de Q-values? \n",
    "* Welk model behaalt doorgaans een hogere totale score? \n",
    "* Leg aan de hand van de plots uit waar de verschillen vandaan komen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.d (4 punten)\n",
    "\n",
    "Laten we nu verder kijken naar de verschillende leermodellen (lees: combinaties van parameterwaardes). Je zal in de vorige onderdelen hebben ervaren dat er veel random noise is. Als gevolg van de random noise kan het gebeuren dat in één leerepisode de beste kist niet gevonden wordt. Het is daarom goed om naar het gemiddelde te kijken van meerdere runs om een beter beeld te krijgen van een specifiek algoritme.  \n",
    "\n",
    "Schrijf nu een loop die `q_learn` 500 keer aanroept met een bepaalde parametersetting (en nog altijd 200 leerrondes) en sla telkens de totale score op, zodat je aan het eind een lijst hebt van 500 totale scores.\n",
    "\n",
    "Vergelijk het gemiddelde van de 500 totaalscores voor $\\alpha=0.1$, $\\alpha=0.3$ en $\\alpha=0.5$. Gebruik $\\epsilon=0.1$ voor alle experimenten. Verklaar hoe de verschillen tot stand komen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# II. Exploration-Exploitation\n",
    "\n",
    "## 2.  $\\epsilon$-greedy\n",
    "\n",
    "We gaan nog wat dieper in op het exploration vs. exploitation dilemma. De robot gebruikt nu de meest simpele exploratieregel: $\\epsilon $-greedy. Laten we kijken hoe verschillende waardes voor  $\\epsilon$ uitwerking hebben op de totale score die behaald wordt. \n",
    "\n",
    "Gebruik hier de functie `q_learn` met $\\epsilon=0.05$, $\\epsilon=0.2$ en $\\epsilon=0.6$. Gebruik $\\alpha=0.3$ voor alle experimenten. \n",
    "\n",
    "### Q2.a (4 punten)\n",
    "\n",
    "Kijk voor elke parametersetting weer naar de gemiddelde totaalscore van 500 leer-episodes. Waar ligt ongeveer het optimale niveau van exploratie? Probeer te verklaren waarom de alternatieven minder goede resultaten opleveren (bijvoorbeeld waarom is een te hoge of juist te lage $\\epsilon$ niet goed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.b (4 punten)\n",
    "In Machine Learning en Deep Neural Nets wordt nog steeds vaak gebruik gemaakt van $\\epsilon$-greedy, ook al is dit niet een exploratieregel die mensen lijken te gebruiken. Ook hier is het kiezen van een goede waarde voor $\\epsilon$ vaak het resultaat van trial en error. Het kan dus zo zijn dat de onderzoeker niet de optimale waarde kiest. Een regel die vaak wordt toegepast is dat de waarde van $\\epsilon$ afneemt gedurende het experiment. \n",
    "\n",
    "Pas nu `q_learn` zo aan dat de parameter $\\epsilon$ gedurende een leer-episode (dus gedurende 200 trials) steeds kleiner wordt. Verschillende manieren zijn mogelijk, maar doe dit in deze opgave door na elke trial $\\epsilon$ met een vast percentage te verkleinen (denk aan iets tussen 0 en 10%). Sla dit model op als `q_learn_decay()`. Schrijf de functie zodanig dat het invullen van bijvoorbeeld `decay = 0.05` ertoe leidt dat $\\epsilon$ na elke trial met 5% wordt verlaagd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_decay(alpha, epsilon, trials=200, decay=0):\n",
    "    # TO DO\n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.c (4 punten)\n",
    "\n",
    "Onderzoek weer de gemiddelde totaalscore over 500 leer-episodes, maar nu voor `q_learn_decay`. Verken waarden van $\\epsilon$ tussen .1 en .9. Gebruik hierbij wederom 200 rondes per leer-episode, en gebruik $\\alpha =  0.3$ en `decay = 0.05`. Voor welke waarden van $\\epsilon$ presteert `q_learn_decay` het best? Waarom denk je dat dat zo is? Kun je voor het behalen van een hoge totaalscore beter `q_learn` of `q_learn_decay` gebruiken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Softmax\n",
    "\n",
    "Een andere, zeer populaire, methode om met het exploratie-explotatie dilemma om te gaan is de softmax beslisregel. Deze kan worden gebruikt om de kans uit te rekenen dat de robot een specifieke kist kiest. De kans dat de robot op een bepaald moment kist i kiest is:\n",
    "\n",
    "$$P(\\text{kist i}) = \\frac{\\exp(Q(i)\\cdot\\theta)}{\\sum_s \\exp(Q(s)\\cdot\\theta)}, \\qquad \\text{voor alle } i \\in \\{1, 2, 3, 4\\}$$\n",
    "\n",
    "De notatie $\\exp(x)$ staat hier voor $e^x$. Natuurlijk geldt op ieder moment $P(\\text{kist 1}) + P(\\text{kist 2}) + P(\\text{kist 3}) + P(\\text{kist 4}) = 1$. De robot kiest immers altijd één van de 4 opties, dus bij elkaar opgeteld moeten de kansen 100% zijn.\n",
    "\n",
    "De waarde van $\\theta$ bepaalt de mate waarin het verschil in verwachte waarden (Q-waarden) de kans op een bepaalde keuze vergroot. Deze parameter wordt ook wel de *inverse temperature* genoemd. Als de waarde van $\\theta$ *laag* is, is de temperatuur van de functie *hoog* en dan doet het verschil in Q-waarden er niet zoveel toe, dus worden er meer random keuzes gemaakt. Oftewel, er zal meer geëxploreerd worden. Dat getuigt van een zekere buigzaamheid, we zijn dan immers minder toegewijd aan een specifieke keuze. De term temperatuur komt van de analogie met het smeden van ijzer: bij een hoge temperatuur is ijzer makkelijker te buigen. \n",
    "\n",
    "### Q3 (6 punten)\n",
    "\n",
    "Implementeer nu de softmax-regel door de code die je hebt geschreven voor de functie `q_learn` te kopiëren en aan te passen. Geef je nieuwe functie de naam `q_learn_softmax`.\n",
    "\n",
    "Gebruik voor `q_lean_softmax` weer de gemiddelde totaalscore van 500 leer-episodes om verschillende waardes van $\\theta$ met elkaar te vergelijken (tenminste vijf verschillende waardes tussen $0.01$ en $1$), met wederom 200 rondes per leer-episode en $\\alpha=0.3$.\n",
    "\n",
    "Wat is ongeveer de optimale waarde voor $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_softmax(alpha, theta, trials=200):\n",
    "    # TO DO \n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)\n",
    "\n",
    "# TO DO: vergelijk parameter waardes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen nu weten waar de verschillen vandaan komen. Laat zowel voor $\\theta$ = $0.1$ als $\\theta = 1$ zien wat de Q-waarden en totaalscore zijn na een leer-episode. Plot voor diezelfde leer-episode ook weer de keuzeratio's per 10 trials (zelfde als bij Q1.c).\n",
    "\n",
    "__Let op:__ We hebben te maken met toevalsexperimenten, dus iedere keer dat je je code runt zullen de Q-waarden, totaalscores en plots anders zijn. Run je code daarom meerdere keren en rapporteer de output van een leer-episode die jij representatief vindt voor alles wat je langs hebt zien komen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat laten deze waardes en grafieken je zien?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Upper Confidence Bound\n",
    "\n",
    "De softmax-regel is vaak al beter dan $\\epsilon$-greedy, maar exploratie is nog steeds random. Als we kijken naar het gedrag van mensen, zien we dat mensen slim zijn en niet random verkennen (exploreren). Een zeer populaire methode om niet geheel random exploratie te implementeren is met Upper Confidence Bound exploratie. \n",
    "\n",
    "Volgens deze regel krijgen de mogelijke acties niet alleen een $Q$-waarde, maar ook een $Q'$-waarde. Steeds wordt de actie gekozen met de hoogste $Q'$-waarde. Als meerdere acties de hoogste $Q'$-waarde hebben, dan wordt random één van deze acties gekozen. De $Q'$-waarde van een actie wordt gegeven door haar $Q$-waarde *plus* een bonus $\\eta$ om exploratie te stimuleren. Deze bonus maakt het model nieuwsgieriger. Hier is een populaire versie genaamd UCB1:\n",
    "\n",
    "$$Q'(action) = Q(action)+ \\eta(action)$$\n",
    "\n",
    "$$\\eta(action) = \\sqrt{\\frac{\\theta \\cdot \\log(t)}{N_t(action)}}$$\n",
    "\n",
    "Hierbij staat $t$ voor het rondenummer in het experiment (zero-based indexing), en $N_t$ voor het aantal keer dat de actie tot dusver gekozen is. De parameter $\\theta$ schaalt de grootte van de exploratiebonus (met een hogere $\\theta$ is de robot nieuwsgieriger). Bovendien: hoe minder vaak er voor $action$ is gekozen, hoe lager $N_t(action)$, hoe hoger $\\eta(action)$, hoe hoger $Q'(action)$. Op deze manier wordt exploratie van weinig gekozen acties dus inderdaad gestimuleerd.\n",
    "\n",
    "**Let op:** het algoritme kiest aan het begin van het experiment elke actie minstens één keer, en daarna wordt pas gekozen op basis van de hoogste $Q'$-waarde. In de huidige context wordt dus in de ronde met index $0,1,2,3$ respectievelijk kist $1,2,3,4$ gekozen, en daarna gaat de hierboven beschreven keuzeregel van start. \n",
    "\n",
    "### Q4.a (6 punten)\n",
    "\n",
    "Implementeer nu de UCB1 exploratieregel op basis van de `q_learn` functie en geef deze de naam `q_learn_UCB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def q_learn_UCB(alpha, theta, trials=200):\n",
    "    # TO DO                  \n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reken voor `q_learn_UCB` weer de gemiddelde totaalscore voor 500 leer-episodes uit voor $\\theta = 2$ (default voor UCB1), met wederom 200 rondes per episode en $\\alpha = 0.3$.\n",
    "* Plot ook weer de keuzeratio's per 10 trials (zoals bij Q1.c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO gemiddelde\n",
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Als je kijkt naar de implementatie, hoe verhoudt dit model zich tot de simpele versie van $\\epsilon$-greedy en soft-max? \n",
    "* Kijk ook naar het aantal punten dat het behaalt, waar komt het verschil in prestatie vandaan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.b (6 punten) \n",
    "De exploration-exploitation trade-off wordt al lange tijd onderzocht en is een van de meest interessante problemen in reinforcement learning. In dit voorbeeld was het best makkelijk voor het algoritme om de oplossing te vinden omdat de 4 schatkisten redelijk veel verschilden van elkaar, maar dit is niet altijd het geval. Kijk maar eens wat er gebeurt als je de standaarddeviatie in `q_learn_UCB` van 4 naar 10 verandert, of als je de means dichter bij elkaar zet. Dan zal je ook zien dat je een hogereexploratie waarde $\\theta$ nodig hebt om tot de beste oplossing te komen.\n",
    "\n",
    "* Verander de SD en means van de schatkisten: <br>\n",
    "    `means = [20, 30, 35, 45]`<br>\n",
    "    `std_dev = 10`\n",
    "* Simuleer nu weer voor verschillende waardes van $\\theta$ het algoritme voor 500 leer-epsiodes met ieder 200 trials (verken tenminste vier $\\theta$-waardes tussen $2$ en $50$);\n",
    "* Rapporteer de gemiddelde totaalscores en de beste $\\theta$-waarde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_UCB_new(alpha, theta, trials=200):\n",
    "    # TO DO                     \n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samenvatting\n",
    "\n",
    "In de simpele wereld van 4 schatkisten (jargon: 'bandits') is nog met enige moeite uit te vinden welk niveau van exploratie optimaal is, maar dit kost veel tijd. Daarnaast is de echte wereld nog veel complexer, dus dan is het helemaal moeilijk om uit te zoeken wat het beste niveau van exploratie is. Het blijft dus altijd een vraag hoe slimme systemen bepalen hoeveel of weinig zij gaan exploreren. Bij mensen lijken hier ook grote individuele verschillen in te zitten: de een is avontuurlijker dan de ander en mensen passen hun exploratiegedrag aan op de omgeving.\n",
    "\n",
    "#### Curiosity-based exploration in complex systems\n",
    "Voor een veel complexer maar interessant voorbeeld is het leuk om te kijken naar: [Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/). In dit paper probeert een agent de omgeving zo te verkennen dat het op zoek gaat naar states waar het meest over te leren valt, net als UCB. Door alleen heel nieuwsgierig te zijn zorgt het algoritme er al voor dat het heel ver kan komen in bepaalde games, zonder dat beloningen (bv punten) daar een belangrijke rol in spelen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# III. Model fitting\n",
    "\n",
    "## 5. De vier schatkisten, menselijke keuzes\n",
    "\n",
    "We gaan nu kijken naar de resultaten van een echt experiment. We hebben de data van een proefpersoon die het bovenstaande experiment met de vier kisten echt heeft gespeeld (let op: wel met andere means). In de *L4_data_1.txt* file kunnen we terugvinden welke van de 4 opties zij gekozen had en hoeveel punten er vervolgens in elke trial verdiend zijn. We gaan kijken welke parameterwaardes het gedrag van de proefpersoon het beste voorspellen als we gebruik maken van Q-learning met een softmax keuzeregel. De proefpersoon heeft 80 rondes gespeeld. \n",
    "\n",
    "Lees de data in uit *L4_data_1.txt* (staat in de Data map) met behulp van [loadtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html):\n",
    "```python\n",
    "with open(\"Data\\L4_data_1.txt\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\"\\t\", skiprows=1)\n",
    "```\n",
    "\n",
    "Mocht je moeite hebben met het inlezen van het bestand, probeer dan (ook van toepassingen voor latere inlees-code in deze notebook):\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(\"Data\") / \"L4_data_1.txt\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\"\\t\", skiprows=1)\n",
    "```\n",
    "\n",
    "We pasen de `q_learn_softmax` functie aan zodat je deze op de data van de proefpersoon kan fitten (zie eerdere opdracht over hoe functies gefit moeten worden en code onderaan voor meer hulp).\n",
    "\n",
    "We gaan ervan uit dat de proefpersoon enige ervaring heeft met dit type experiment en verwachten dat ze gemiddeld wel 40 punten per ronde gaat verdienen (alle Q’s starten op 40 ipv 0). Voor het fitten van het model maken we gebruik van `minimize` van scipy.optimize (`from scipy.optimize import minimize`) en we gaan proberen de *Log Likelihood (LL)* te optimaliseren (= maximaliseren). \n",
    "\n",
    "Wat we bij elke trial willen weten is de kans dat het model dezelfde keuze maakt als de proefpersoon. Hoe groter de kans (likelihood) dat het model correct kiest, hoe beter het model \"fit\". \n",
    "\n",
    "In het databestand van de proefpersoon kun je zien welke van de 4 kisten de proefpersoon koos. Dit kunnen we elke ronde vergelijken met de corresponderende keuzekansen volgens het model. In de eerste ronde zijn alle Q values nog gelijk dus $P(\\text{kist 1}) = P(\\text{kist 2}) = P(\\text{kist 3}) = P(\\text{kist 4}) =  0.25$. Voor de eerste ronde geldt daarom automatisch dat de kans (likelihood) dat de keuze van het model overeenkomt met de keuze van de proefpersoon ook 0.25 is, maar dat gaat veranderen naarmate er geleerd wordt.\n",
    "\n",
    "De output van deze functie moet de som van alle $\\log(P(\\text{kist gekozen}))$ zijn. Let op, deze som wordt vermenigvuldigd met -1 omdat minimize de functie probeert te *minimaliseren*, en we zijn op zoek naar het *maximum* van de log likelihood. \n",
    "\n",
    "`**example code**:`\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "with open(\"Data\\L4_data_1.txt\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\"\\t\", skiprows=1)\n",
    "\n",
    "def q_learn_softmax_fit(params):\n",
    "    alpha, theta, init_value = params\n",
    "    nArms = 4\n",
    "    Q = np.array([init_value]*nArms)\n",
    "    LL = 0\n",
    "    \n",
    "    # loop over all trials\n",
    "    for row in data:\n",
    "        \n",
    "        # bereken de kans (probs) dat elke kist gekozen wordt\n",
    "        probs = np.power(np.e, theta*Q)\n",
    "        probs /= sum(probs)\n",
    "        \n",
    "        choice = row[1]-1   # de keuze van de proefpersoon\n",
    "        outcome = row[2]    # de uitkomst van de keuze\n",
    "        \n",
    "        # bereken de log likelihood van de keuze van de proefpersoon (choice)\n",
    "        # Gegeven de huidige Q values van het model, hoe groot was de kans (likelihood) \n",
    "        # dat deze specifieke kist gekozen werd?\n",
    "        LL += np.log(probs[choice]) # houd de som van alle LL bij. \n",
    "        \n",
    "        # Update de Q-values, gegeven de werkelijke keuze en ook de werkelijke outcome \n",
    "        # gebruik daarbij vrije parameter alpha, de learning rate, we proberen de optimale waarde hiervan te vinden\n",
    "        Q[choice] = Q[choice] + alpha * (outcome - Q[choice])\n",
    "    \n",
    "    # Schaal met -1 om de functie te kunnen minimizen\n",
    "    return -1*LL\n",
    "\n",
    "# minimize takes a few arguments (function, array of initial parameter values, minimization methods,\n",
    "# bounds are the bounds on each parameter; use bounds (same, same) to fix parameter to a single value\n",
    "res = minimize(q_learn_softmax_fit, \n",
    "               np.array([0.5, 0.5, 40]), \n",
    "               method='SLSQP',\n",
    "               bounds=[(0,1), (0,10), (40,40)], \n",
    "               options={'disp':True, 'ftol':1e-16})\n",
    "```\n",
    "\n",
    "### Q5.a (7 punten)\n",
    "\n",
    "* Gebruik deze functie en fit het model. \n",
    "* Welke parameterwaarden fitten de data van de proefpersoon het beste? \n",
    "* Wat kan je zeggen over hoe goed het model de keuzes van de proefpersoon voorspelt? (Geef gemiddelde likelihood per trial en wat dat betekent.) <br>\n",
    "\n",
    "    _Minimize geeft de uiteindelijke summed LL (negatief) van het best fittende model. Deze score kan je weer terugrekenen naar een gemiddelde likelihood  per trial: de kans dat het model juiste trial koos. Doe dit en beoordeel de uitkomst. Hints: deel de output door het aantal trials, gebruik de inverse $e^x$ van $\\log(x)$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Q5.b (4 punten)\n",
    "\n",
    "Als we de functie een klein beetje herschrijven kunnen we deze gebruiken om in het hoofd van de proefpersoon te kijken. Nu willen we bijvoorbeeld weten welke Q-values zij aan de verschillende kisten toekent. \n",
    "\n",
    "Hiervoor is alleen een kleine verandering nodig, waarbij de functie nu niet meer de Log Likelihood als output heeft maar de lijst met Q-values. Pas de functie `q_learn_softmax_fit` aan zodat deze de Q-values returnt en noem de nieuwe functie `q_learn_fitted_model`. Zorg hierbij dat de `params` lijst van argumenten ook hetzelfde blijft als bij de `q_learn_softmax_fit`.\n",
    "\n",
    "Nu hoef je ook niet meer de functie te fitten of minimalizeren maar alleen aan te roepen, gebruikmakende van de beste gevonden `params` van de vorige stap.\n",
    "\n",
    "`q_learn_fitted_model(res.x)`\n",
    "\n",
    "Rapporteer de Q-values van deze proefpersoon. In werkelijkheid waren de gemiddelde beloningen van de kisten (50, 30, 20, 80). Hoe wijkt de proefpersoon hier van af en hoe is dat te verklaren? (Hint: kijk naar het keuzegedrag in de data file.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_fitted_model(params):\n",
    "    # TO DO\n",
    "    return Q\n",
    "\n",
    "q_learn_fitted_model(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Two-armed Bandits\n",
    "\n",
    "\n",
    "We gaan nu kijken naar de resultaten van een ander experiment. Net als bij het vorige experiment is er telkens een keuze tussen opties die verschillende beloningen opleveren. In het eerdere voorbeeld waren het schatkisten, maar in de meeste experimenten zijn dit gokkasten (ook wel bandits genoemd omdat ze uiteindelijk met je geld er vandoor gaan). \n",
    "\n",
    "<img src=\"Images\\2bandits.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "In deze versie van het experiment heeft de proefpersoon telkens de keuze tussen twee machines. Anders dan de kisten, geven deze machines wel of geen beloning. De ene machine geeft de beloning met een grotere kans (80%) dan de andere (20%). \n",
    "Net als hierboven kan je in een data file (*RL_data2.csv*) terugvinden welke van de opties de proefpersoon gekozen heeft en hoeveel punten er verdiend zijn. Er waren in totaal 8 paren van bandits, dus 16 objecten in totaal. \n",
    "\n",
    "Lees de data in uit *RL_data2.csv* (staat in de Data map) met behulp van [loadtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html):\n",
    "```python\n",
    "with open(\"Data\\RL_data2.csv\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\",\", skiprows=1)\n",
    "```\n",
    "De headers in de file geven aan welke data er in de kolommen te vinden is: <br>\n",
    "`playerID` is de identificatie van de speler, `trial` is het trial nummer, `outcome` is de uitkomst van de keuze, `pairz` geeft aan welke paar zichtbaar was, waar paar 1 bestaat uit stimulus (object) 1 en 2, paar 2 uit stimulus 3 en 4, etc. Dan staat er in de kolom `choice` welke van de twee objecten gekozen is.\n",
    "\n",
    "Schrijf een nieuwe `q_learn_softmax_fit` functie zodat je deze op de data van de proefpersonen kan fitten. Ga er van uit dat de proefpersonen _geen_ ervaring heeft met dit type experiment en verwachten dat hij/zij gemiddeld  0 punten per ronde gaat verdienen (alle Q’s starten op 0). \n",
    "\n",
    "### Q6.a (5 punten)\n",
    "\n",
    "Fit het aangepaste model op de data van deze taak. Let er goed op dat voor het berekenen van de probabiliteit van het kiezen van een bepaalde stimulus je alleen rekening moet houden met de Q value van het alternatief binnen hetzelfde paar, niet de Q values van alle andere stimuli (zoals hierboven). Dus als pair 5 wordt aangeboden zie je alleen stim 9 en 10 en moet je alleen rekenen met $Q(9)$ en $Q(10)$.\n",
    "\n",
    "* Rapporteer de waardes van $\\alpha$, $\\theta$ en de Log Likelihood. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvements\n",
    "\n",
    "Deze simpele 2-armed Bandit taak wordt heel veel gebruikt om onderzoek te doen bij mensen en dieren. Deze taak is bijvoobeeld bij mensen en dieren afgenomen om leren in het brein te kunnen bestuderen. Het is een simpele taak, maar deze heeft veel inzichten gegeven in hoe mensen leren, inzichten die soms ook weer vertaald zijn naar de ontwikkeling van bijvoorbeeld DNNs en andere toepassingen in de kunstmatige intelligentie.\n",
    "\n",
    "Verbeteringen en uitbreidingen op het leermodel zijn vaak geïnspireerd door observaties van gedrag. Zoals we bij model fitting hebben geleerd, worden deze modellen getoetst door te kijken of de uitbreidingen ook echt leiden tot een betere fit. Hier ga je een aantal van deze verbeteringen bedenken en uitwerken. \n",
    "\n",
    "### Q6.b Dual Learning rates (5 punten)\n",
    "Het blijkt dat mensen en dieren anders reageren op beloning en straf (zie ook colleges week 1). Dat is iets wat niet in het standaard Q-learning model meegenomen is. Schrijf nu een versie van q-learning die dat wel doet: `q_learn_softmax_fit_dual`. Gezien de titel van deze vraag kan je al raden dat dit kan door twee verschillende learning rates ($\\alpha_{gain}$ en $\\alpha_{loss}$) te introduceren. Het makkelijkste is om voor deze opdracht deze twee learning rates te associeren met positive en negatieve prediction errors.\n",
    "\n",
    "* Rapporteer de waardes van $\\alpha_{gain}$, $\\alpha_{loss}$, $\\theta$ en de Log Likelihood. \n",
    "* Is dit model inderdaad een betere fit? Wat geeft het verschil in learning rates aan? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_softmax_fit_dual(params):\n",
    "    # TO DO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.c Meta Learning (6 punten)\n",
    "Iets anders wat opvalt bij het leerpatroon van mensen is dat ze langzaam maar zeker de structuur van de taak zelf door krijgen. Dat wordt ook wel meta-leren genoemd. Ook dit is een belangrijk onderdeel van leren, en krijgt wederom veel aandacht binnen de AI gemeenschap (zie ook Lake et al. paper). Want door de structuur van een taak te leren kan er sneller en beter geleerd worden in toekomstige trials, maar deze kennis kan ook van toepassing zijn in nieuwe vergelijkbare situaties. \n",
    "\n",
    "Denk bijvoorbeeld aan het spelen van Super Mario. In het eerste level leer je veel over de structuur van het spel, paddestoelen zijn bijvoorbeeld goed en schildpadden slecht. Handige informatie voor in het volgende level, maar ook voor in een soortgelijk spel: Sonic the Hedgehog (in AI onderzoek zijn deze oude spelen weer helemaal populair). \n",
    "\n",
    "De two bandits taak is erg simpel, maar er zit wel een structuur in, namelijk: van elk paar is altijd één van de twee bandits goed en de ander slecht. Ook verschillen de kansen op positieve en negatieve feedback maar minimaal. Als je feedback krijgt over één van de twee opties in een paar, leer je eigenlijk ook al iets over de ander. \n",
    "\n",
    "Jouw opdracht is om het algoritme aan te passen op een manier dat het deze kennis van de taak gebruikt. Laat zien hoe het model fit op de data fit. Het gaat nu meer om de implementatie van de kennis over de taak, het is niet erg als het model minder goed fit. \n",
    "\n",
    "* Schrijf een nieuw algortime\n",
    "* Motiveer je keuzes\n",
    "* Evalueer je model (parameter waardes, en LogLikelihood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.d Dynamic Learning rates (5 bonus punten)\n",
    "We zien vaak dat mensen in dit soort taken steeds minder reageren op de feedback, sommigen helemaal niet meer. Als je eenmaal weet welke van de twee objecten de beste is, dan moet je eigenlijk de negatieve uitkomsten die soms komen gewoon negeren. Klassieke Q-learning modelen die gebaseerd zijn op het Rescorla-Wagner model kunnen dat gedrag niet goed vangen. \n",
    "\n",
    "We hadden eerder al gezien dat het Pearce-Hall model een andere insteek had wat betreft de learning rate, namelijk dat deze afneemt naarmate de associatie sterkte toeneemt. Het gebeurt nu nog erg weinig maar dit kan ook binnen Q learning geimplementeerd worden.\n",
    "\n",
    "* Implemeneer de PH dynamic learning rate in het single learning rate Q-learning algoritme\n",
    "Voor het gemak:\n",
    "```python\n",
    "alpha = gamma*(abs(outcome - Q[choice]))+((1-gamma)*alpha)\n",
    "```\n",
    "\n",
    "Zorg dat je de scalar $S$ meeneemt in het model! Doe dit voor gemak in het single learning rate model. Wordt het er beter op? Wat is de waarde van gamma, en wat betekent dat? Vergelijk de fit van dit model dan dus ook met het single learning rate model zonder dynamic learnig rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_pearce_hall(params):\n",
    "    # TO DO\n",
    "    return -1*LL\n",
    "\n",
    "# TO DO: fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
