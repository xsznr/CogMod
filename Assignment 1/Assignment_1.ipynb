{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Cognitive Modelling <font color='red'>Deadline = 18 april 23:59</font>\n",
    "\n",
    "Dit is de eerste opdracht voor Cognitive Modelling. Hieronder volgen vragen en opdrachten over verschillende leermodellen. Bij elke vraag staat de hoeveelheid punten die je er voor kan krijgen. In totaal kan je voor deze opdracht __94__ punten halen.\n",
    "\n",
    "Geef antwoorden in blokken met code of met tekst. Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hier om wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig. \n",
    "\n",
    "Sla het uiteindelijke notebook __met al gerunde output__ op, met jullie studentnummers en achternamen in de filenaam: `studentnummer1_achternaam1_studentnummer2_achternaam2_opdrachtnummer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dit is een voorbeeld voor het antwoord bij uitlegvragen, hier kan je eventueel `code` gebruiken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib plotting basics\n",
    "\n",
    "Hieronder een aantal links naar tutorials en documentatie voor het gebruik van `matplotlib` voor het maken van grafieken.\n",
    "\n",
    "* [Pyplot tutorial](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "* [Axis labels](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.ylabel.html)\n",
    "\n",
    "* [Adding a legend](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html)\n",
    "\n",
    "*  zie [hier](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) voor een markdown cheatsheet, en [hier](https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols) voor een handige lijst $\\LaTeX$ symbolen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these to initialize libraries\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pt. 1 Het Rescorla-Wagner model\n",
    "\n",
    "De Rescorla-Wagner theorie illustreert hoe leermethoden gevoelig kunnen zijn voor de link tussen een geconditioneerde stimulus (CS) en een ongeconditioneerde stimulus (US). In het experiment van Pavlov leert een hond de associatie tussen een belletje en het krijgen van vlees. \n",
    "\n",
    "Een hond produceert normaliter speeksel als voedsel (US) wordt aangeboden. Na het herhaaldelijk horen van een belletje voordat de hond het vlees krijgt, zal de hond leren dat dit belletje (CS) gevolgd wordt door het vlees. De hond zal vervolgens na het horen van het belletje al speeksel produceren. Het effect van de geconditioneerde stimulus (het belletje) wordt groter als er vaker een koppeling wordt gepresenteerd met een ongeconditioneerde stimulus (het vlees): hoe vaker de associatie aangeboden wordt, hoe groter de respons en hoe groter dus de associatieve sterkte ($V$ in het Rescorla-Wagner model). \n",
    "\n",
    "Het Rescorla-Wagner model vormt de basis voor alle Reinforcement Learning modelen die veelvuldig gebruikt worden in kunstmatige intelligentie. Met dit model kun je sterktes van associaties kwantificeren, en begrijpen hoe deze veranderen met tijd. Ook kunnen we verschillende simulaties met het model doen om te kijken wat voor gedrag het wel, maar ook **niet** voorspelt. Deze simulaties kunnen ons inzicht geven tot op welke hoogte een model de onderliggende cognitieve processen goed beschrijft. \n",
    "\n",
    "Meer details over het Rescorla-Wagner model, extinctie, compound stimuli en blocking kan je vinden in *Learning & Memory*, en in de college slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Associatieve sterktes\n",
    "\n",
    "Stel dat de CS (het belletje üîî) en de US (het vlees üçñ) 10 keer worden aangeboden. De hond had vooraf geen enkele associatie met het belletje, dus de sterkte van de associatie voordat de CS en US de eerste keer worden gepresenteerd is 0: $V_{t=0} = 0$. \n",
    "\n",
    "De maximale sterkte die de associatie tussen de CS en US kan hebben is 100: $V_{max} = 100$, en de leersnelheid is 0.4: $\\alpha = 0.4$. \n",
    "\n",
    "Om het simpel te houden gaan we in de opdrachten uit van een simpel RW-model met maar √©√©n learning rate:\n",
    "\n",
    "$$ V_{t+1} = V_t + \\Delta V_t $$\n",
    "$$ \\Delta V_t = \\alpha(V_{max} - V_t) $$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Q1.a (4 punten)\n",
    "Het belletje wordt nu voor de eerste keer aangeboden en de hond krijgt de beloning. Wat is de waarde van $V$ en $\\Delta V$ na de eerste leertrial? Dus op $t = 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.b (4 punten)\n",
    "\n",
    "Schrijf een functie die de waardes van $V$ en $\\Delta V$ kan uitrekenen en bijhouden voor meerdere leertrials. Zorg er ook voor dat je kan aangeven welke associatie er eventueel al is tussen de stimulus en beloning. Noem de functie `rescorla_wagner()`.\n",
    "\n",
    "Om je op weg te helpen:\n",
    "```python\n",
    "def rescorla_wagner(alpha, v_start, v_max, trials):\n",
    "    v_list = [V_0] # list with V_0\n",
    "    delta_list = [‚àÜV_0] # list with delta V_0\n",
    "    \n",
    "    for _ in range(trials):\n",
    "        #1 calculate V_t\n",
    "        #2 store V_t in list Vs\n",
    "        #3 calculate ‚àÜV_t \n",
    "        #4 store ‚àÜV_t in list ‚àÜVs\n",
    "\n",
    "    return (v_list, delta_list)\n",
    "```\n",
    "\n",
    "**Let op!** Als je de waardes *na* 5 leertrials wilt berekenen, zal deze functie 6 waardes teruggeven voor $V$ en $\\Delta V$, omdat $V_0$ en $\\Delta V_0$ ook zijn toegevoegd. Dat is de bedoeling. De eerste waarde, met index = 0, betekent na 0 leertrials. De tweede waarde, met index = 1, betekent na 1 leertrial, etc. Dus *na* 10 trials, gebruik je index = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescorla_wagner():\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c (3 punten)\n",
    "\n",
    "Gebruik de functie om het leren van een associatie in 10 trials te simuleren. Gebruik hiervoor dan een learning rate van 0.4 ($\\alpha$), een associatie van nul om te beginnen ($V$), en een $V_{max}$ van 100. Wat zijn de berekende waardes voor $V$ en $\\Delta V$ na 10 trials? Hoe interpreteer je deze waardes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Interpretatie*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.d (3 punten)\n",
    "We gaan nu een aantal grafieken maken om het gedrag van het model beter te begrijpen door de leercurves te laten zien. Om je op weg te helpen is hier een functie om de grafieken weer te geven:\n",
    "\n",
    "``` python\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trials(x_label, y_label, line_labels, *results_list):\n",
    "    trials = range(len(results_list[0]))\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    for result in results_list:\n",
    "        plt.plot(trials, result)\n",
    "    \n",
    "    plt.legend(line_labels)\n",
    "    plt.show()\n",
    "```\n",
    "De eerste twee argumenten gebruik je om de x- en y-as te labelen. Je kan bijvoorbeeld `x_label = r\"$t$\"` en `y_label = r\"$V$\"` gebruiken om aan te geven dat het om $t$ trials op de x-as en $V$ associatie waardes op de y-as gaat.\n",
    "\n",
    "Plot nu de grafiek van de hele leercurve voor 10 trails van de vorige opgave ($\\alpha$ = 0.4, $V_{t=0}$ = 0, $V_{max}$ = 100). Gebruik hiervoor de output van `rescorla_wagner()` en gebruik `plt.hlines(95, 0, 10, colors='red')` voordat je `plot_trials()` aanroept om een horizontale lijn op 95% toe te voegen. Met het argument `line_labels` kan je een label aan je leercurve toevoegen. Je kan `line_labels = []` gebruiken als je geen gebruik wil maken van deze optie.\n",
    "\n",
    "Na hoeveel trials is de associatieve sterkte op 95% van de maximale associatieve sterkte? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.e (4 punten)\n",
    "\n",
    "Bereken nu ook de associatieve sterktes $V$ voor andere waardes van $\\alpha$ (0.1 en 0.6). Maak een plot met de twee leercurves voor de verschillende waardes van $\\alpha$. Met `line_labels = [r\"$\\alpha = 0.1$\", r\"$\\alpha = 0.6$\"]` kan je ervoor zorgen dat beide lijnen de juiste labels krijgen.\n",
    "\n",
    "Wat is het effect van een hogere of lagere $\\alpha$ op $V$?\n",
    "\n",
    "Plot in een aparte grafiek ook de $\\Delta V$'s, hoe verschillen die voor de verschillende $\\alpha$'s ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Effect van $\\alpha$ op $V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot delta V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Effect van $\\alpha$ op $\\Delta V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Extinctie curves\n",
    "\n",
    "Nu ga je een extinctie curve plotten. Volg de stappen hieronder om de juiste waardes van $V$ te kunnen uitrekenen. In geval van extinctie begint de hond met al een sterke associatie tussen bel en beloning ($V_{t=0} > 0$), maar deze associatie dooft uit. Gebruik dat $V = 100$ de associatie is aan het begin tussen de stimulus en de beloning. Wat er nu gaat gebeuren is dat de bel rinkelt en de hond niks krijgt (maar dat dus wel verwacht).\n",
    "\n",
    "### Q2.a (2 punten)\n",
    "\n",
    "Wat is de associatie waarde $V_{max}$ bij extinctie trials?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.b (2 punten)\n",
    "\n",
    "Bereken nu de waarde van $V$ na de eerste extinctie trial met behulp van `rescorla_wagner()` (gebruik $\\alpha$ = 0.4). Wat is deze waarde $V_{t=1}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.c (3 punten)\n",
    "\n",
    "Bereken nu de waardes van $V$ voor twintig extinctie trials en plot deze in een grafiek. Na hoeveel extinctie trials is de associatieve sterkte $V$ minder dan 1% van de associatieve sterkte waar je mee begon? Gebruik weer de horizontale lijn om dit goed te kunnen zien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.d (5 punten)\n",
    "\n",
    "Plot ook de absolute waardes van $\\Delta V$ in een aparte grafiek, wat gebeurt daar mee?\n",
    "\n",
    "Als je $\\Delta V$ zou moeten omschrijven als een cognitief of psychologisch proces, wat zou dat zijn? Waarom is het aan het begin van extinctie groot en later kleiner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Compound stimuli\n",
    "\n",
    "De basis van het Rescorla-Wagner model is duidelijk. Nu gaan we kijken naar het Rescorla-Wagner model voor compound stimuli. Bij compound stimuli bestaat de CS niet uit 1 stimulus maar uit een samenstelling van meerdere stimuli, bijvoorbeeld A (belletje) en B (lampje). Zoals in het college besproken, wordt de maximale associatieve waarde ($V_{max}$) van de US dan verdeeld over de stimuli A en B (etc.). \n",
    "\n",
    "Het idee hierachter is dat het associatieproces wordt gestuurd door aandacht. De stimulus waar de meeste aandacht aan wordt besteed, of die de meeste aandacht vraagt, zal een hogere learning rate krijgen. Daarom zullen stimuli die meer opvallen een sterkere associatie krijgen. Het is mogelijk dat de aandacht die een stimulus krijgt ervoor zorgt dat associaties beter of langer in het geheugen worden opgeslagen.\n",
    "\n",
    "Aandacht is een hele effici√´nte manier om leren te versnellen. Het implementeren van aandacht speelt ook een belangrijke rol in de ontwikkelingen van deep neural networks. Als je hier meer over wilt lezen kijk dan eens naar https://arxiv.org/abs/1802.04712 of https://dl.acm.org/doi/10.1145/3055635.3056621. We weten nog steeds niet precies hoe aandacht werkt, maar deze papers laten zien hoe het mogelijk ge√Ømplementeerd kan worden in DNN's en hoe dat het leren kan versnellen!\n",
    "\n",
    "Neem voor de volgende opdracht aan dat er twee stimuli zijn: A üîî en B üí°. Stimulus A (het belletje) is indringender dan stimulus B (het lampje), dus A krijgt meer aandacht en heeft een hogere leersnelheid ($\\alpha = 0.4$) dan stimulus B ($\\alpha = 0.1$). Neem aan dat $V_{max}$ weer 100 is en $V_{t=0}=0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Q3.a (5 punten)\n",
    "Maak √©√©n grafiek met de associatieve sterktes van A ($V_A$), B ($V_B$) en de compound van A en B ($V_A + V_B$), drie lijnen in totaal. Schrijf hiervoor een nieuwe functie `rescorla_wagner_cs()` en plot deze drie associatieve sterktes voor 20 leertrials. Baseer deze functie op je originele `rescorla_wagner()`. \n",
    "\n",
    "**Let op!** Als het goed is kan $V_A+V_B$ nooit meer zijn dan $V_{max}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescorla_wagner_cs():\n",
    "    # TO DO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.b (4 punten)\n",
    "\n",
    "Bij het begin van het experiment zijn alle associatie sterktes $V$ = 0. Wat zijn de associatie sterktes van $V_A$, $V_B$ en $V_A + V_B$ na de eerste trial? En wat zijn de waardes na 20 trials?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Blocking\n",
    "\n",
    "Ga er bij het volgende experiment van uit dat er weer twee stimuli zijn: een belletje A üîî en een lampje B üí°. Dit keer is het lampje even duidelijk als het belletje (zet voor beide stimuli $\\alpha = 0.2$). Alleen nu zal bij dit blocking experiment alleen stimulus A in de eerste fase worden aangeboden en daarna na een tijd beide stimuli tegelijk:\n",
    "\n",
    "fase 1:  CS$_A$ $\\rightarrow$ US (20 trials)\n",
    "\n",
    "fase 2:  CS$_A$ + CS$_B$ $\\rightarrow$ US (20 trials)\n",
    "\n",
    "### Q4.a (3 punten)\n",
    "\n",
    "Bij het begin van het experiment zijn alle associatie sterktes 0. Wat is de associatie sterkte $V_A$ na de eerste trial in fase 1? En van $V_B$ na de eerste trial in fase 1? En beide waardes na de tweede trial in fase 1? \n",
    "\n",
    "**Let op!** je kan hier gebruik maken van de functie die je voor opdracht 3 over compound stimuli hebt geschreven: `rescorla_wagner_cs()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.b (5 punten)\n",
    "\n",
    "Bereken nu de associatieve sterktes van $V_A$ en $V_B$ voor de 20 trials in fase 1, en dan voor de 20 trials in fase 2. Maak √©√©n grafiek met de associatieve sterktes van $V_A$ en $V_B$ voor deze 40 trials in totaal. \n",
    "\n",
    "Wat gebeurt er met de associatieve sterktes van stimulus B? Hoe zal de hond reageren als er na deze 40 trials alleen een lampje aangaat zonder bel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Latent inhibition\n",
    "\n",
    "In het volgende experiment zijn er twee honden. De eerste hond, Schnauzie üêï, wordt in een hok gehouden waar af en toe, zonder goede reden, een belletje af gaat (üîîCS). Als het belletje gaat gebeurt er verder niks. De tweede hond, Bello üê©, wordt in een hok gehouden waar helemaal nooit iets te horen valt. Vervolgens doen beide honden mee aan het experiment van Pavlov waarbij een belletje (üîîCS) aan een beloning (ü¶¥Ô∏èUS) wordt gekoppeld.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Q5.a (5 punten)\n",
    "\n",
    "Simuleer de data voor beide honden en plot de learning curves van pre-exposure en leertrials samen in √©√©n grafiek ($\\alpha = 0.4$). Gebruik 10 pre-trials voor de pre-exposure van Schnauzie ($V_{max}=0$), en dan 10 leertrials voor beide honden (nu wel een beloning dus $V_{max}$ = 100). \n",
    "\n",
    "Je kan voor de pre-exposure en leertrials `rescorla_wagner()` gebruiken om de waardes van $V$ uit te rekenen, of je kunt een nieuwe functie schrijven.\n",
    "\n",
    "Leg uit wat je ziet in de grafiek, klopt dit met wat je hebt geleerd over pre-exposure leercurves?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.b (4 punten)\n",
    "\n",
    "Hier de data van een echt pre-exposure effect:\n",
    "<img src=\"Images\\curves.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Hoe verschillen jouw learning curves met die van een echt pre-exposure effect? Wat is daar een mogelijke verklaring voor? Hoe zou jij het Rescorla-Wagner model aanpassen om dit te ondervangen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pt. 2 Pearce-Hall error learning theory\n",
    "\n",
    "Een van de meest voor de hand liggende oplossingen van *latent inhibition* is dat de learning rates verschillen. Het Pearce Hall leermodel (PH) kan onder andere latent inhibition verklaren. Als je hier meer over wilt lezen: Pearce J.M. and Hall G. (1980) [A model for Pavlovian learning: Variations in the effectiveness of conditioned but not of unconditioned stimuli. Psychological Review](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=bfde4e5ddaa6a968df9984b557baccd7cb38fb82).\n",
    "\n",
    "__Let op!__ De notatie in deze opgave wijkt een klein beetje af van de notatie in de hoorcollege slides. Voor het oplossen van deze opdracht gebruik je de notatie zoals hieronder beschreven.\n",
    "\n",
    "Net als in het RW model geldt dat $V_{t+1} = V_t + \\Delta V_t$. In tegenstelling tot het RW model wordt volgens het PH model de verandering in de associatieve sterkte ($\\Delta V_t$) echter bepaald door de volgende vergelijking:\n",
    "\n",
    "$$\\Delta V_t = S * \\alpha_t * V_{max}$$\n",
    "\n",
    "De scalar $S$ wordt bepaald door de intensiteit van de CS en door de intensiteit van de US. De parameter $\\alpha$ vertegenwoordigt de _associability_ van de CS en wordt verondersteld <u>hoog</u> te zijn voor nieuwe stimuli. De parameter $S$ zagen we in het college ook als parameter $\\beta$ bij het RW model. De intensiteit kan dus ook weer in verband worden gebracht met aandacht: hoe intenser, hoe meer aandacht en hoe sneller het leren gaat. \n",
    "\n",
    "In tegenstelling tot de $\\alpha$ in het RW-model is $\\alpha$ in het Pearce-Hall-model niet statisch maar dynamisch. Voor het RW model is de learning rate $\\alpha$ constant en bepaalt deze alleen de grootte van de verrassing (*prediction error*), dus of er veel of weinig geleerd wordt (ofwel hoe groot $\\Delta V$ is). Bij het PH-model kan de  _associability_ rate $\\alpha$ juist veranderen. Deze wordt bepaald door:\n",
    "\n",
    "$$ \\alpha_{t} = |V_{max} - V_t| \\qquad (t \\geq 1)$$\n",
    "\n",
    "Dat wil zeggen, de waarde van $\\alpha$ die wordt gebruikt om $V_{t+1}$ te berekenen wordt bepaald door de absolute waarde van het verschil tussen $V_{max}$ en de associatie sterkte na leertrial $t$. Dit betekent dat een grote (absolute) prediction error ervoor zorgt dat er sterk geleerd wordt op _de volgende_ trial. Dus een grote verassing zorgt dat er in de toekomst sneller geleerd wordt. \n",
    "\n",
    "Bij eenvoudige *conditionering*, waarbij een CS betrouwbaar is gekoppeld aan een US, verhoogt $V$ op basis van ervaring  en  daalt $\\alpha$ (nadert nul naarmate asymptoot in leren wordt bereikt). De scalar $S$ wordt wel verondersteld statisch te zijn, en lijkt in die zin dus meer op de learning rate in het RW model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.6a (6 punten)\n",
    "Schrijf een functie `pearce_hall()` die een lijst van $\\Delta V$ waarden, een lijst van $V$ waarden, en een lijst van $\\alpha$ waarden als output heeft, zodat je kan bijhouden hoe deze waardes per trial veranderen.\n",
    "\n",
    "Om je op weg the helpen:\n",
    "\n",
    "`def pearce_hall(S, alpha=0.7, v_start=0, v_max=1, trials=10):` \n",
    "\n",
    "oftewel, de input voor het model is scalar $S$, $\\alpha$, $V = 0$ en $V_{max}$ = 1. De beginwaarde voor $\\alpha$ wordt niet berekend met de bovenstaande formule, maar wordt als input meegegeven. De inputwaarde is in dit geval hoog ($\\alpha_0 = 0.7$) omdat de stimuli aan het begin nieuw zijn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearce_hall():\n",
    "    # TO DO \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot met behulp van deze functie de curves voor $\\Delta V$, $V$ en $\\alpha$ in drie aparte grafieken voor 10 trials met verschillende waardes van $S$: 0.2 en 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vergelijk deze grafieken met de leercurves van het RW model; \n",
    "* Beschrijf wat de verschillende waardes van $S$ voor invloed hebben op $\\Delta V$, $V$ en $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Laat vervolgens in een plot zien dat pre-exposure wel invloed heeft, en je dus wel een (klein) effect van latent inhibition ziet. (Tip! leer het model eerst voor 10 trials met $V_{max}=0$ ga dan verder met de $\\alpha$ die je dan hebt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pearce-Hall and beyond\n",
    "\n",
    "Onlangs is er bewijs gevonden voor zowel PH- als RW-leersignalen in het menselijk brein, wat suggereert dat het onderliggende algoritme aspecten van beide integreert. Het kan zijn dat grote prediction errors leiden tot een directe impact op het leren van de huidige gebeurtenis (grote $\\Delta V$), maar ook de aandacht richten op toekomstige gebeurtenissen (vergroten van $\\alpha$) en zo het leren bij volgende gebeurtenissen vergroten (of juist de aandacht verminderen omdat er een tijdje niets interessants gebeurt en de $\\alpha$ verlagen). \n",
    "\n",
    "De Pearce Hall modellen waren wat minder populair dan het Rescorla Wagner model, dus veel van huidge Reinforcement Learning modellen hebben geen dynamische learning rates. Toch is dat erg belangrijk, en je ziet ook dat dit soort modellen weer in opkomst zijn. Voor het oplossen van bepaalde Reinforcement Learning problemen gaan kunstmatige intelligentie onderzoekers vaak gewoon een paar learning rates uitproberen om te kijken welke het beste werkt. Op basis van de inzichten uit deze voorbeelden zouden sommige modelen dus makkelijker verbeterd kunnen worden door dynamische learning rates toe te voegen. \n",
    "\n",
    "Suggesties om verder te lezen:\n",
    "\n",
    "Pearce and Hall (1980) [A model for Pavlovian learning: Variations in the effectiveness of conditioned but not of unconditioned stimuli. Psychological Review](https://pdfs.semanticscholar.org/3630/3957caa2ebda4d04b6f25334d4fe9bf4b3cf.pdf) <br>\n",
    "Roesch et al. (2002) [Surprise! Neural Correlates of Pearce-Hall and Rescorla-Wagner Coexist within the Brain](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3325511/#R92) <br>\n",
    "Behrens et al. (2007) [Learning the value of information in an uncertain world](https://www.nature.com/articles/nn1954) <br>\n",
    "Piray & Daw (2020) [A simple model for learning in volatile environments](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007963#pcbi.1007963.ref006)\n",
    "\n",
    "\n",
    "### Q.6b (2 punten)\n",
    "Kan je bedenken in wat voor omgeving een dynamische learning rate handig kan zijn en wanneer juist niet? Je kan inspiratie opdoen in Behrens et al. (2007) en  Piray & Daw (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pt. 3 Temporal Difference Learning \n",
    "\n",
    "Temporal difference learning is een uitbreiding op het RW model dat ook de rol van tijd en geheugen in het koppelen van de CS en de US kan beschrijven. Het is een simpele uitbreiding van het Rescorla Wagner model en vormt tot op de dag van vandaag de basis van de meeste reinforcement learning algoritmes. \n",
    "\n",
    "Tijd is heel erg belangrijk omdat in de meeste gevallen beloningen van ons gedrag ver in de toekomst liggen. Denk bijvoorbeeld aan het spel Go. Pas aan het eind, als de winnaar bekend is gemaakt, komt er een beloning. We zouden niet willen zeggen dat alleen de laatste zet de winnende zet was, en dus alleen die geassocieerd moet worden met de beloning. Alle zetten hebben op een of andere manier meegeholpen aan de overwinning. Een van de grootste computationele uitdagingen is om ook deze zetten goed te associ√´ren met de beloning. Dit heet ook wel het *credit assignment problem*. Temporal Difference Learning is een eerste, simpele, maar krachtige zet in die richting.\n",
    "\n",
    "Details over het TD model zijn terug te vinden in de college slides, het hoofdstuk van Gureckis & Love [**computational reinforcement learning**](http://bradlove.org/papers/GureckisLovePress.pdf) en voor meer verdieping in het online boek van [**Sutton & Barto**](http://incompleteideas.net/book/bookdraft2018jan1.pdf) en dan met name hoofdstuk 6 en 12.\n",
    "\n",
    "Het leren in deze opdracht speelt zich af in een simpel **Markov Reward Process** met de volgende structuur:\n",
    "\n",
    "![](Images\\random_walk.png)\n",
    "\n",
    "De robot start elke episode in $C$, en gooit dan telkens een muntje op om te bepalen welke kant hij op gaat (kop = links, munt = rechts, beide 50% kans). Zodra de robot √©√©n van de vierkante eindpunten bereikt is de episode (het spel) over en begint de robot weer bij punt $C$. \n",
    "\n",
    "Alleen als de robot in het rechter eindpunt eindigt krijgt hij 1 punt, alle andere beloningen zijn nul. Toch zal de robot op een gegeven moment doorhebben dat als hij naar rechts beweegt, hij steeds op een betere plek terecht komt. We kijken in dit voorbeeld nog even niet naar keuzes, er wordt immers telkens een muntje opgegooid, maar we brengen eerst in kaart hoe er een associatie kan komen tussen verschillende states en toekomstige beloningen. \n",
    "\n",
    "In het begin van het experiment heeft de robot geen enkele kennis van de wereld en geen enkele verwachting voor elke staat:\n",
    "\n",
    "$$V(A)=V(B)=V(C)=V(D)=V(E)= 0, \\quad\\text{idem voor de vierkante eindpunten}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Het TD-model\n",
    "\n",
    "Zoals gezegd is het TD model gebaseerd op het RW model, met de aanpassing dat het de factor tijd kan meenemen. De volledige formulering van het TD model is:\n",
    "\n",
    "$$V(s_t) = V(s_t) + \\alpha [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$$\n",
    "\n",
    "Deze formule schrijft voor hoe de verwachting voor de staat waar we waren op tijdstip $t$ moet worden ge√ºpdatet op basis van waar we zijn beland op tijdstip $t + 1$. __Let op: deze formule staat verkeerd in de hoorcollege slides.__ Daar lijkt de update-regel voor $V(s_{t+1})$ te zijn, maar het is echt $V(s_t)$ die we moeten updaten. Verder is de uitleg over dit model en deze formule uit het hoorcollege erg nuttig, dus kijk de opname over dit stukje nog eens terug voordat je deze opgave maakt. Zie ook [**computational reinforcement learning**](http://bradlove.org/papers/GureckisLovePress.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7.a (2 punten)\n",
    "\n",
    "Stel de robot speelt dit spel voor de eerste keer en begeeft zich nu in state E. ![](Images\\random_walk_reward.png)\n",
    "\n",
    "In het begin is de verwachte waarde van staat E nul ($V(E)=0$). Nu gooit de robot het muntje op en stapt voor het eerst naar rechts en verdient $1$ punt.\n",
    "\n",
    "Wat is hierna de verwachte waarde $V(E)$?\n",
    "\n",
    "* Gegeven $\\alpha = 0.1$ en $\\gamma =1$? \n",
    "* En voor $\\alpha = 0.6$ en $\\gamma =1$?\n",
    "\n",
    "Reken uit aan de hand van bovenstaande formule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7.b (6 punten)\n",
    "\n",
    "Schrijf een functie `td_zero()` die als input, alpha ($\\alpha$) , gamma ($\\gamma$) en het aantal episodes accepteert. De output van deze functie moet een lijst met values $V$ zijn voor elke staat (A t/m E) na het leren van een aantal  episodes.\n",
    "\n",
    "\n",
    "##### Pseudo code:\n",
    "* Initieer de staat van de wereld:\n",
    "    * Er zijn 7 states (5 letters & 2 endpoints), voor alle $V_{t=0} = 0$\n",
    "* Cre√´er een `for loop` over alle episodes\n",
    "    * Elke episode begint in state C\n",
    "* In elke episode blijf je lopen tot je een van de eindpunten bereikt `while loop`\n",
    "    * Gooi muntje op, en maak een stap, houd bij in welke state je bent.\n",
    "    * Kijk wat je beloning is na maken van stap en update $V_t$\n",
    "    * Eindpunt bereikt? Be√´indig de episode en start een nieuwe, zo niet weer muntje opgooien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero():\n",
    "    # TO DO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laat met behulp van deze functie de robot 100 episodes leren over deze wereld. Hoe zien zijn verwachtingen eruit voor elke state ($V$)\n",
    "- Gegeven $\\alpha$ = 0.1 en $\\gamma$ = 1? \n",
    "- En voor $\\alpha$ = 0.6 en $\\gamma$ = 1? \n",
    "\n",
    "Welke is dichter bij de waarheid? (Lees eerst even de volgende opgave voordat je de huidige opgave beantwoordt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. MSE\n",
    "\n",
    "Nu willen we weten hoe goed deze waardes die de robot toekent overeenkomen met de werkelijke staat van de wereld. Analytisch kan men bepalen dat dit de verwachte waarden moeten zijn als de robot voor altijd in dit universum zou ronddolen:\n",
    "\n",
    "$$ V(A)=\\frac{1}{6}\\ ,\\ V(B)=\\frac{2}{6}\\ ,\\ V(C)=\\frac{3}{6}\\ ,\\ V(D)=\\frac{4}{6}\\ ,\\ V(E)=\\frac{5}{6}$$\n",
    "\n",
    "Net als bij vele statistische modellen en machine learning algoritmes beoordelen we deze leeruitkomsten niet op basis van absolute verschillen van deze waardes, maar op basis van mean squared error ($MSE$). De error is in dit geval dus het verschil tussen de verwachte waarde van een staat volgens de robot, en de analytisch-verwachte waarde.\n",
    "\n",
    "Gebruikmakende van de $MSE$ functie die je hieronder gaat schrijven, kunnen we onderzoeken hoe snel het algoritme dicht bij de echte waardes terecht komt, en welke parameter waardes voor $\\alpha$ en $\\gamma$ het snelst en/of het beste benaderen.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Q8.a  (5 punten)\n",
    "\n",
    "Schrijf nu een functie `MSE()` die de mean squared error berekent tussen de berekende verwachte waardes en de analytisch bepaalde verwachte waardes. Roep de functie `td_zero()` meerdere keren aan in een loop, elke keer met een ander aantal episodes (oplopend). Bereken dan telkens $MSE$, zodat je de veranderingen in $MSE$ per aantal episodes kan volgen. Nu kan je verschillende leercurves met elkaar vergelijken. Als het goed is wordt elke episode de verwachte waarde van de states beter benaderd, en wordt de $MSE$ dus steeds kleiner. Overigens hebben we ook te maken met toeval vanwege de muntworpen, dus er zou wel een daling in $MSE$ zichtbaar moeten zijn, maar er zal ook sprake zijn van ruis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE():\n",
    "    # TO DO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot de leercurves voor $\\alpha = 0.75$ en $\\alpha = 0.15$ voor 1 tot 100 episodes in dezelfde grafiek ($\\gamma=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Beschrijf welk model het beste is in het begin en welke aan het eind. Waardoor komt dat? Is een hoge learning rate altijd beter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8.b  (5 punten)\n",
    "\n",
    "Plot nu de leercurves voor $\\gamma=1$, en $\\gamma=0.5$ voor 1 tot 100 episodes ($\\alpha = 0.1$).  Beschrijf hoe de leercurves lopen, en laat ook de uiteindelijke $V$ waardes voor elk model zien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wat is het effect van $\\gamma$? \n",
    "* Als je kijkt naar de uiteindelijke $V$'s hoe ziet de robot met $\\gamma=0.5$ de wereld (in psychologische termen)? In wat voor een situatie zou dat goed zijn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Eligibility traces                                       \n",
    "\n",
    "Tot nu toe onthield de robot alleen waar hij precies een stap geleden was. Dit maakt leren nog niet heel effici√´nt. Het kan nog steeds lang duren voordat states (of latere handelingen) credits krijgen terwijl ze ook erg dicht bij de beloning waren. Eligibility traces is een eerste stap om TD uit te breiden en om ook states die dichtbij het einddoel zijn credit te geven voor de uiteindelijke beloning (of het uitblijven daarvan). Het is wederom een zeer simpel en logisch proces, maar het maakt het leren veel effici√´nter.\n",
    "\n",
    "![](Images\\random_walk_memory.png)\n",
    "\n",
    "Je kan het vorige algoritme `td_zero()` zien alsof de robot alleen de vorige state kon onthouden. We breiden daarom nu het geheugen uit en daarmee kan hij langer dingen onthouden, en dus ook waardes toekennen aan states die langer in het verleden liggen. \n",
    "\n",
    "Dit gaat door middel van eligibility traces (zie college). Elke staat die recentelijk werd bezocht komt nu ook in aanmerking (is eligible) voor een value update zodra er een beloning valt. Hoe langer een staat niet bezocht is, des te minder deze in aanmerking komt voor een update. Hoe snel een staat in het verleden geen credit meer krijgt (niet meer eligible is) wordt geregeld via de $\\lambda$ parameter:\n",
    "\n",
    "$$ e_t(s)= \\begin{cases}\n",
    "\\gamma \\lambda e_{t-1}(s) & if\\ s \\neq s_t\\\\\n",
    "1 & if\\ s = s_t \\end{cases} $$\n",
    "\n",
    "Net als bij `td_zero()` heeft de laatst bezochte staat altijd eligibility waarde 1. Voor alle hiervoor bezochte states wordt de eligibility in waarde vermindert met parameter $\\lambda$ en discount factor $\\gamma$ (beide tussen 0 en 1).\n",
    "\n",
    "In het nieuwe model worden dan de verwachte waardes van __alle__ states $s$ die eligible zijn (een $e(s) > 0$) ge√ºpdatet met de volgende regel:\n",
    "\n",
    "$$\\Delta V(s) = \\alpha [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)] e(s)$$\n",
    "\n",
    "en dan natuurlijk:\n",
    "\n",
    "$$V(s) = V(s) + \\Delta V(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.a  (7 punten)\n",
    "\n",
    "Implementeer eligibility traces in het `td_zero()` model, en maak er een nieuwe functie van `td_lambda()`. Hiervoor is het nodig om:\n",
    "\n",
    "* Elke episode te beginnen met een lijst met eligibility values (EV), die begint met een eligibility van 0 voor voor alle states (A-E). \n",
    "* Deze lijst moet elke stap worden bijgehouden. Elke keer als de robot in een state komt krijgt deze EV = 1, maar hoe langer de robot niet meer in die state komt des te lager de EV wordt.\n",
    "* Let op! De prediction error (PE) moet nu gebruikt worden om __alle__ states te updaten met een eligibility waarde groter dan nul.\n",
    "* Zorg dat de output van de functie een lijst is van states $V$ en een lijst van eligibility waardes $e$ (een lijst per episode). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda():\n",
    "    # TO DO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run de`td_lambda()` functie die je net gemaakt hebt met ($\\lambda=0.5$) en zonder ($\\lambda=0$) eligibility traces voor slechts 1 episode (episodes = 1, $\\alpha = 0.2$, $\\gamma = 1$);\n",
    "* Rapporteer de lijst van values $V$ en de lijst van eligibilities $e$ voor beide algoritmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wat is het verschil tussen de twee verschillende $\\lambda$ waardes en waarom? Merk op dat we nog altijd te maken hebben met randomness, dus om een helder beeld te krijgen helpt het voor jezelf om je code meerdere keren te runnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.b  (5 punten)\n",
    "\n",
    "Illustreer nu het effect van verschillende waardes van $\\lambda$ op de leercurve (wederom uitgedrukt in $MSE$). \n",
    "\n",
    "Run het `td_lambda() ` model voor 1 tot 200 episodes voor $\\lambda=0.0$, $\\lambda=0.5$, en $\\lambda=0.9$ (voor alle modellen $\\alpha=0.05$ en $\\gamma=1$) en maak een grafiek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschrijf aan de hand van de grafiek wat het effect van $\\lambda$ op leren is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
