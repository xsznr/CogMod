{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmpn_RIjBYex"
   },
   "source": [
    "# Assignment 3c Cognitive Modelling\n",
    "\n",
    "Dit is onderdeel van de derde opdracht voor Cognitive Modelling, assignment 3 bestaat in totaal uit 3 onderdelen:\n",
    "\n",
    "* Assignment 3a (47 punten)\n",
    "* Assignment 3b (19 punten)\n",
    "* __Assignment 3c (23 punten)__\n",
    "\n",
    "__Let op__: Opdracht 3b en 3c werken alleen in Google Colab!\n",
    "\n",
    "Geef antwoorden in blokken met code of met tekst.Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hier om wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig.\n",
    "\n",
    "Sla het uiteindelijke notebook op met jullie studentnummers en achternamen in de filenaam: `studentnummer1_achternaam1_studentnummer2_achternaam2_opdrachtnummer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdE_YiOPBYe0"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Voorbereiding stap 1. De Colab environment\n",
    "\n",
    "Koppel je notebook aan een GPU: ga naar **Edit > Notebook settings** of **Runtime > Change runtime type** en selecteer GPU as Hardware accelerator.\n",
    "\n",
    "![screenshot](https://drive.google.com/uc?id=1QsJXUPPL0eEoWHI7a-A1_F42djdgCcBL)\n",
    "\n",
    "and daarna:\n",
    "\n",
    "![screenshot](https://drive.google.com/uc?id=1ODvXSxF7OBy9blmZfC1Yk870uw80c6YW)\n",
    "\n",
    "Nu moet je jouw Google Drive koppelen aan deze omgeving zodat je daarvan kan lezen en naar kan schrijven. Het zal misschien nodig zijn om een key in te voeren. Klik daarvoor op de link die verschijnt. Die brengt je naar een google pagina, en copy paste dan je key hier in het notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2023,
     "status": "ok",
     "timestamp": 1714475147106,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "L3yuN9hAt_zZ",
    "outputId": "d40177d9-ec43-4c52-b7e0-f006e3ed82ea"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# verander alles achter \"/content/drive/My\\ Drive/\" naar de locatie van de bestanden.\n",
    "%cd /content/drive/My\\ Drive/STA_KI/Assignment\\ 3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1714475147368,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "Dv5Sh21nIY9N",
    "outputId": "73cb9dd9-fcdd-4ce9-effa-a1297d6320d4"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714475147368,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "mZr2J8KAdOZ6",
    "outputId": "a2042400-e021-43a7-b247-13023bb8042a"
   },
   "outputs": [],
   "source": [
    "ls Images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymG6yi_PIiSS"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Voorbereiding stap 2. Installeer Vizdoom ‚òï\n",
    "\n",
    "Nu moeten we Vizdoom en een paar dependencies installeren op de Colab. Deze twee stappen kunnen enkele minuten duren (tijd voor koffie). Let op dat als je de colab instance een tijdje niet gebruikt, je deze stap opnieuw moet doen! Net als de key for google drive geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7915,
     "status": "ok",
     "timestamp": 1714475155281,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "6O0s51tOvA8w",
    "outputId": "3e6b095b-dbc5-4f61-9c00-5d2dbb900d7f"
   },
   "outputs": [],
   "source": [
    "## to get vizdoom to work\n",
    "## https://stackoverflow.com/questions/50667565/how-to-install-vizdoom-using-google-colab\n",
    "## takes a few minutes....\n",
    "%%bash\n",
    "# Install deps from https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
    "apt-get update\n",
    "\n",
    "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
    "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
    "libopenal-dev timidity libwildmidi-dev unzip\n",
    "\n",
    "# Boost libraries\n",
    "apt-get install libboost-all-dev\n",
    "# Lua binding dependencies\n",
    "apt-get install liblua5.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15822,
     "status": "ok",
     "timestamp": 1714475171101,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "viY56QAlvW5c",
    "outputId": "fe117792-9801-4b04-a6da-0f4737904002"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dan installeren we vizdoom, sk-image and imageio in the python environment\n",
    "!pip install vizdoom\n",
    "!pip install scikit-image\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdHqJHvNBYe7"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Voorbereiding stap 3. Setting up the Game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1714475171102,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "me_BN5EKJR0n"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from vizdoom import *\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "executionInfo": {
     "elapsed": 1577,
     "status": "ok",
     "timestamp": 1714475172675,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "yJyK8v5PvmFr",
    "outputId": "196db0f9-bf27-4a03-ec5a-459e3c814b46"
   },
   "outputs": [],
   "source": [
    "# plaatje van vizdoom\n",
    "from IPython.display import Image\n",
    "Image('Images/doom.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdSJuQgtBYe9"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## De regels van het spel üéÆ\n",
    "Nu we alle libraries hebben, kunnen we de regels van het spel bepalen. Doom wordt bepaald door:\n",
    "  - Een  `configuration file` \"basic.cfg\" daar in vinden alle opties (frame size, mogelijke handelingen)\n",
    "  - Een `scenario file`: \"basic.wad\" deze genereerd het correcte scenario (ruimte, monsters etc, komt bij vizdoom) .\n",
    "\n",
    "Open beide files met een text editor en bestudeer ze kort, kijk of je de belangrijkste elementen kan ontcijferen (en later mogelijk veranderen). Het spel ziet er als volgt uit:\n",
    "\n",
    "__De omgeving:__\n",
    "- Een monster staat op een willekeurige plek aan de overkant van het veld\n",
    "- De speler heeft drie mogelijke acties: naar links, naar rechts en schieten\n",
    "- 1 hit is genoeg om het monster uit te schakelen\n",
    "- Een episode is over als het monster is geraakt of door een timeout (300).\n",
    "\n",
    "__De rewards:__\n",
    "- -5 punten voor het schieten van kogel  \n",
    "- -1 punt voor elke tijdstap (energy consumption)\n",
    "- +100 punten voor het uitschakelen van het monster uitschakelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1714475172675,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "Wlrlq2K_vxJL"
   },
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "\n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "\n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "\n",
    "    ## we cant watch the game perform on collab\n",
    "    game.set_window_visible(False)\n",
    "    game.init()\n",
    "\n",
    "    # Here are our possible actions\n",
    "    # The input for Vizdoom needs our\n",
    "    # action space to be defined by one-hot\n",
    "    # encoding.\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions_vizdoom = [left, right, shoot]\n",
    "\n",
    "    # In pytorch we will simply represent our actions\n",
    "    # as integers. Our action space which we will use\n",
    "    # as input for pytorch is this encoded as follows:\n",
    "    left = 0\n",
    "    right = 1\n",
    "    shoot = 2\n",
    "    possible_actions_pytorch = [left, right, shoot]\n",
    "\n",
    "    return game, possible_actions_vizdoom, possible_actions_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCb0fgbaBYe-"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing ‚öôÔ∏è\n",
    "\n",
    "Preprocessing is een belangrijke stap, omdat we de complexiteit van onze data willen verminderen om zo de rekentijd die nodig is voor training te verminderen. In termen van leren kan je deze stap zien als een leraar die de leerling alleen vertelt in welke richting te kijken. Minimale supervision, maar dit helpt de leerling wel door de aandacht te vestigen in de juiste richting.\n",
    "\n",
    "__Preprocessing stappen:__\n",
    "- Grijstinten maken van alle frames (omdat kleur geen belangrijke informatie toevoegt). Dit wordt al gedaan door het configuratiebestand (!).\n",
    "- Het scherm bijsnijden (in ons geval verwijderen we het dak omdat het geen informatie bevat)\n",
    "- Het normaliseren van de pixelwaarden\n",
    "- Het verkleinen van het preprocessed frame.\n",
    "\n",
    "Dit resulteert in een 84x84 grayscale frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1714475172675,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "RqqwSex9x-k1",
    "outputId": "cdea7f04-72fa-4b3e-ddf2-fea12da5711b"
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "\n",
    "    # The grayscaling is already done in our vizdoom config\n",
    "    # So no need to do that here, if we would have wanted to\n",
    "    # we could have used this functions:\n",
    "    # from skimage.color import rgb2gray\n",
    "\n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "\n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "\n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "\n",
    "    return preprocessed_frame\n",
    "\n",
    "Image('Images/preproc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6XFk7lEBYe_"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Stacked Frames üïñüïóüïòüïô\n",
    "\n",
    "Als input states gaan we dus nu geen single frames gebruiken, maar een stapeltje (stacked) frames. Het stapelen van frames is erg belangrijk omdat het een gevoel van beweging/tijd meegeeft aan het neurale netwerk. Eerst preprocessen we het frame, vervolgens voegen we het frame toe aan een *deque* dat automatisch het oudste frame verwijdert, dan bouwen we de state (S) van deze frames.\n",
    "\n",
    "Dit is hoe de functie werkt:\n",
    "- Voor de start state voeren we 4 keer hetzelfde begin frame in\n",
    "- Bij elke tijdsstap voegen we het nieuwe frame toe aan de deque, en wordt het oudste frame weggehaald. Deze nieuwe stack is dan de nieuwe state\n",
    "- Enzovoort\n",
    "- Als een episode voorbij is maken we een nieuwe stapel met 4 nieuwe frames (omdat we ons in een nieuwe episode bevinden).\n",
    "\n",
    "Hier is nog een meer in depth artikel over stacked frames:  <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  Frame skipping and preprocessing</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1714475172675,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "WtL6H0TO5x2t"
   },
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, state, is_new_episode,stack_size=4):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_episode:\n",
    "        # Clear stacked frames and repeat the initial frame\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int_) for _ in range(stack_size)], maxlen=stack_size)\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        # Add the latest frame and automatically remove the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "    # Stack the frames along a new dimension\n",
    "    # This is where we change the original code\n",
    "    # making sure the channels are (C,H,W)\n",
    "    # We can add the last channel later\n",
    "    stacked_state = np.stack(stacked_frames, axis=0)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRe0wx-SBYfA"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Initialiseren van hyperparameters üéõ\n",
    "In dit deel stellen we de verschillende hyperparameters in. Eerst definieren we de hyperparameters van het neurale netwerk. Om de mogelijke acties die we in het spel kunnen doen op te halen uit het spel zullen we eerst vizdoom opstarten. Dit kunnen we doen door onze create_environment functie op te starten, loop nog een keer door deze functie heen om precies te zien wat die doet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "p6LXAHjXHhne"
   },
   "outputs": [],
   "source": [
    "game, possible_actions_vizdoom, possible_actions_pytorch = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "Fjl5y-ZFBYfB"
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [4,84,84]\n",
    "action_size = game.get_available_buttons_size()    # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0025      # Alpha (aka learning rate)\n",
    "batch_size = 64  # Batch size for replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgCyHcKnBYfB"
   },
   "source": [
    "### ‚ùì __Q1 (2 punten)__\n",
    "Waarom is de input voor het model, `State_size`,  een matrix van 4x84x84?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Drf898icBYfC"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1soJUzKBYfC"
   },
   "source": [
    "---\n",
    "\n",
    "## Deep Q-learning Neural Network model bouwen üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8rWlJIzBYfC"
   },
   "source": [
    "Dit is de structuur van het Deep Q-learning model (zie ook de afbeelding hieronder):\n",
    "- We nemen de stacked frames as input\n",
    "- Deze input gaat door 3 convnets\n",
    "- De data wordt platgeslagen (flattened), en gaat dan door twee fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "R_NRc94FBYfC",
    "outputId": "6a31a0a7-43c6-44a8-d1c7-0476975d54d3"
   },
   "outputs": [],
   "source": [
    "Image('Images/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTiH7hhfBYfD"
   },
   "source": [
    "Bestudeer de code onder vraag 3 bij `class DQNetwork`, kijk in hoeverre je de structuur zoals hier boven beschreven terug kan lezen. Het is nu niet belangrijk dat alle details helemaal begrijpt, maar wel de grote lijnen.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ‚ùì__Q2. Network Questions (4 punten)__\n",
    "\n",
    "1. Hoeveel nodes heeft de laatse laag, en wat representeert deze laag?\n",
    "2. Welke activatie functie gebruikt het netwerk?\n",
    "3. En welke loss functie wordt er geminimalizeerd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNhv3MW9BYfD"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dERy0KROBYfD"
   },
   "source": [
    "<br>\n",
    "\n",
    "Als het goed is heb je bij Computer Vision alles over Convnets geleerd en ook bij andere vakken over ANNs. Maar om toch even kort terug naar de theorie. Waarom gebruiken we hier niet een simpel one-layer network, of waarom geen Q-tables zoals we hiervoor hebben gedaan? De reden is de grootte van de state space.\n",
    "\n",
    "\n",
    "### ‚ùì__Q3. (2 punten)__\n",
    "Hoe groot is de state space (na pre-processing) en hoeveel state, action pairs zijn er dus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmpPA6bVBYfE"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcE6rt_EBYfE"
   },
   "source": [
    "<br>\n",
    "\n",
    "Precies! Dat is al heel erg groot en dit spel is nog steeds erg beperkt. Stel je voor dat je nog door de ruimte kan bewegen etc. We zagen al dat het grotere Frozen Lake van 8x8 lastig was voor Q-tables. Dit is niet te doen. Convnets kunnen de agent de state space laten doorgronden en laten generalizeren.\n",
    "\n",
    "Een convnet kan gebruik maken van spatiele relaties: denk bijvoorbeeld aan dat als het monster twee stappen links van jou staat, je altijd LEFT, LEFT, SHOOT zou moeten doen, ongeacht of het monster helemaal links staat of in het midden. Als je deze relaties kan gebruiken om te leren is dat dus heel handig. Een Q-table aanpak zou voor state monster_links en state monster_midden geheel opnieuw moeten leren wat de beste actie is.\n",
    "\n",
    "Meer lezen over convolutional nets:\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "wL047gyO-0n0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, learning_rate=learning_rate):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=state_size[0], out_channels=32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32,eps=1e-5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64,eps=1e-5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128,eps=1e-5)\n",
    "\n",
    "        # Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "\n",
    "        # Calculate the size of the flattened output after the last convolutional layer\n",
    "        self.fc_input_dim = self._get_conv_output_dim(state_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(in_features=self.fc_input_dim, out_features=512)\n",
    "        self.output_layer = nn.Linear(in_features=512, out_features=action_size)\n",
    "\n",
    "        # Xavier initialization for fully connected layers\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutions\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward pass through the fully connected layer\n",
    "        x = F.elu(self.fc(x))\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _get_conv_output_dim(self, input_shape):\n",
    "        # Dummy pass to get the output shape\n",
    "        return self._forward_features(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        # Helper function to pass input through convolutional layers\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnaKKNfABYfF"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### De hyperparameters voor de training üéõ\n",
    "In dit deel voegen we de trainingshyperparameters toe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "_VziM9FlK0Ec"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "episodes = 501  # Total episodes for training\n",
    "max_steps = 100  # Max possible steps in an episode\n",
    "epsilon_start = 1.0  # Initial exploration probability\n",
    "epsilon_end = 0.01  # Minimum exploration probability\n",
    "epsilon_decay = 0.0001  # Exponential decay rate for exploration prob\n",
    "gamma = 0.95  # Discounting rate for future rewards\n",
    "\n",
    "pretrain_length = batch_size  # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10000  # Number of experiences the Memory can keep\n",
    "\n",
    "training = True  # Modify this to False if you just want to see the trained agent\n",
    "episode_render = False  # Turn this to True if you want to render the environment (visualize the agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF5Nt7mEBYfG"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Experience Replay üîÅ\n",
    "Nu we ons neurale netwerk hebben gemaakt, gaan we de Experience Replay-methode implementeren. Eerst maken we een deque Memory-object. Een deque (dubbele wachtrij) is een gegevenstype dat het oudste element verwijdert telkens wanneer er een nieuw element wordt toegevoegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "QJcKey8_-Cqc"
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        return [self.buffer[i] for i in index], buffer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsC_co0IBYfH"
   },
   "source": [
    "Hier zullen we het probleem aanpakken dat de agents beginnen met een volledig leeg geheugen: we vullen ons geheugen vooraf in door willekeurige acties te ondernemen en de ervaring op te slaan (*state, action, reward, new_state*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1714475243694,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "PYyXtuHxPRpa"
   },
   "outputs": [],
   "source": [
    "def fill_up_memory(memory_size, pretrain_length):\n",
    "\n",
    "    memory = Memory(max_size=memory_size)  # Initialize memory\n",
    "    game.new_episode()  # Start a new episode in the environment\n",
    "\n",
    "    stack_size = 4 # We stack 4 frames\n",
    "    stacked_frames  =  deque([np.zeros((84,84), dtype=np.int_) for i in range(stack_size)], maxlen=stack_size) # Initialize deque with zero-images one array for each image\n",
    "\n",
    "    for i in range(pretrain_length):\n",
    "        if i == 0:\n",
    "            state = np.array(game.get_state().screen_buffer)\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)  # Initial state\n",
    "\n",
    "        action = random.choice(possible_actions_pytorch)  # Choose a random action\n",
    "        reward = game.make_action(possible_actions_vizdoom[action])  # Execute the action, reward is returned\n",
    "        done = game.is_episode_finished()  # Check if episode is done\n",
    "\n",
    "        if done:\n",
    "            # Episode ended\n",
    "            next_state = np.zeros(state.shape) # Fill next state with zeros\n",
    "            memory.add((state, action, reward, next_state, done)) # Add experience to memory\n",
    "\n",
    "            game.new_episode()  # Start a new episode\n",
    "            state = game.get_state().screen_buffer  # Get the new initial state\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)  # Restack frames\n",
    "        else:\n",
    "            # Episode continues\n",
    "            next_state = game.get_state().screen_buffer # Get the next state\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            state = next_state  # Update the current state\n",
    "\n",
    "    return memory, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTHwJVF8q9Qr"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Tensorboard üìä\n",
    "\n",
    "We gebruiken deze functie om de data van de training weg te schrijven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1714475172676,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "3NWWtKDdq-Na"
   },
   "outputs": [],
   "source": [
    "# Initialize the SummaryWriter\n",
    "def initialize_tensorboard(path_tensorboard,run_id):\n",
    "    writer = SummaryWriter(f\"{path_tensorboard}/{run_id}/\")\n",
    "\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulWyDWfrBYfH"
   },
   "source": [
    "Hieronder defini√´ren we de keuzefunctie van de agent. We implementeren een  klassieke e-greedy keuzeregel met exponenti√´le afname in exploratie. Een nieuwigheid is dat er een ondergrens is voor de parameter **explore_stop**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1714475172677,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "DvJeS2I-D5Vm"
   },
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions, model, device):\n",
    "    # Epsilon Greedy Strategy\n",
    "    # Calculate explore probability\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    # Randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # we do a random move based on the explore probability:\n",
    "    if exp_exp_tradeoff > explore_probability:\n",
    "        # Make a random action (explore)\n",
    "        action = random.choice(possible_actions)\n",
    "    # otherwise we get our model to do a move based on the state\n",
    "    else:\n",
    "        # Get action from DQN (exploit)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add a batch dimension and send to device\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            Qs = model(state)\n",
    "        model.train()  # Set the model back to train mode\n",
    "\n",
    "        # output of our model is a factor, containing the probabilities (Q-values)\n",
    "        # for the moves, we choose the one with the highest probability:\n",
    "        # Select the action with the highest Q-value\n",
    "        action = torch.argmax(Qs).detach().item()\n",
    "        state.detach()\n",
    "        del state\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1714475172677,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "7CuAVeNtYhQY"
   },
   "outputs": [],
   "source": [
    "def optimize_model(experiences, policy_net, optimizer, gamma, device):\n",
    "    # first we unpack the experiences, the experiences are drawn from memory\n",
    "    # and are as many as the batch size. Each experience is a tuple containing\n",
    "    # the state, action, reward, next_state and if the episode is done or not.\n",
    "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "    states = torch.FloatTensor(np.array(states)).to(device)\n",
    "    actions = torch.LongTensor(np.array(actions)).unsqueeze(-1).to(device)  # Ensure actions are long and have correct shape\n",
    "    rewards = torch.FloatTensor(np.array(rewards)).to(device)\n",
    "    next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "    dones = torch.FloatTensor(np.array(dones)).to(device)\n",
    "\n",
    "    # Get Q values for the chosen actions, this gathers all the Q values for the actions\n",
    "    # provided in the actions tensor, which correspond to the states in the batch.\n",
    "    current_q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "    # Here we predict the q-values for the next states, given the current policy network\n",
    "    # and we detach the values, so that we do not backpropagate through the target network.\n",
    "    next_q_values = policy_net(next_states).detach().max(1)[0]\n",
    "    # Compute the target Q values for the current states\n",
    "    # based on $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "    # the 1-dones is used to make sure that the target is equal to the reward\n",
    "    # when the episode is done.\n",
    "    target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Delete all the tensors! We do not want to keep them in memory.\n",
    "\n",
    "    del states, actions, rewards, next_states, dones\n",
    "    del current_q_values, target_q_values, next_q_values\n",
    "\n",
    "    return loss.detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoQBnwWsBYfJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Training üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Ons algoritme:\n",
    "<br>\n",
    "* Initialiseer de weigths van het netwerk\n",
    "* Initialiseer de game omgeving\n",
    "* Initialiseer de decay rate voor e-greedy\n",
    "<br>\n",
    "* **For** episode to max_episode **do**\n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate (decreas decay)\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "\n",
    "Je ziet dat dit eigenlijk precies hetzelfde is als we eerder in het FrozenLake Q-net hadden geimplementeerd, alleen zitten er nu dus wel wat lagen tussen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAuxTS3LBYfK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### ‚ùì __Q4.a (7 punten)__\n",
    "\n",
    "Je gaat hieronder de agent trainen in 500 episodes met de huidige set hyperparameters, dit kan wel ongeveer 10 minuten duren, dus tijd om even pauze te nemen en de benen te strekken. Bekijk en rapporteer na afloop het gedrag van de agent tijdens het leerproces. Dit kan op twee manieren:\n",
    "\n",
    "1. Aan de hand van de score die de agent behaalt tijdens het leren;\n",
    "2. Aan de hand van hoe de agent het spel speelt (visueel).\n",
    "\n",
    "__1. Score__\n",
    "  \n",
    "  * Tijdens het trainen schrijft het script de output weg in **test.txt** die kan worden gebruikt voor visualisatie van de leercurve(s). Deze wordt dus in je Google Drive geschreven. Geef deze file een andere naam die specifiek is voor setttings van het netwerk dat je traint, zo kan je later modelen vergelijken.\n",
    "  * Maak twee plots op basis van deze data: de leercurve met de score van de agent tijdens het leren, de kans op een exploratie. Zet in de grafieken de gemiddelde punten en explore probabilities voor bins van 10 episodes om de leercurven te illustreren en rapporteer ook het totaal aantal punten. Beschrijf wat je ziet.\n",
    "\n",
    "__2. Visualisatie__\n",
    "\n",
    "* De code hieronder slaat het model om de 50 episodes `model_XX.ckpt.` op in de directory `models`. Dit is de staat van het model na XX episodes trainen. De filenames specificeren dus weer het model.\n",
    "\n",
    "  Met de code onder vraag 5 kan je de boven genoemde outputfiles gebruiken om naar gedrag van de agent te kijken, gegeven de weights in het netwerk na een een XX aantal leer episodes. Deze code laat slechts √©√©n game zien (√©√©n episode), dus doe dit misschien een aantal keer omdat het anders net zo kan uitvallen dat de game begint dat de agent recht voor de alien staat en meteen neerschiet. Dat is goed maar vertelt weinig over het gedrag.\n",
    "\n",
    "  Naast deze ene game, slaat deze code ook de sequentie van states als lijst van screenshots op in `trajectory`. Deze `trajectory` kan je in een animatie te maken door ze achter elkaar te plakken. Je kan deze  bekijken, en ook opslaan als mp4.\n",
    "  \n",
    "* Beschrijf het verschil in gedrag in drie verschillende fasen (episode 50, 150 en 500). Wat voor soort fouten zie je? Sla de vier .mp4 bestanden van deze drie fases op en __lever de bestanden in__.\n",
    "\n",
    "_Let op: voor sommigen werkt de functie_ `save_path = saver.save(sess, \"./models/model_{}.ckpt\".format(episode)` _niet altijd goed. Het is niet duidelijk waarom, maar dan kan je dit deel van de opdracht helaas niet helemaal doen, laat dit weten aan je TA._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1714475173134,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "jBFekz3kzgeg"
   },
   "outputs": [],
   "source": [
    "# Define the name of your run\n",
    "# for tensorboard and logfiles:\n",
    "path_tensorboard=\"./tensorboard/\"\n",
    "run_id= \"lr-0_02_bs-64_eps_dec-0_0001_gam-0_95\"\n",
    "\n",
    "# initialize tensorboard:\n",
    "writer = initialize_tensorboard(path_tensorboard,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 139266,
     "status": "error",
     "timestamp": 1714475388617,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "wP-HBT1iB2Ja",
    "outputId": "90bc98a8-9953-4655-e044-20a5cdcf6db9"
   },
   "outputs": [],
   "source": [
    "# Setup torch\n",
    "torch.backends.cudnn.enabled = False  # Disable CuDNN for eficient execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize network, optimizer, memory\n",
    "dqn_network = DQNetwork(state_size, action_size).to(device)\n",
    "optimizer = optim.RMSprop(dqn_network.parameters(), lr=learning_rate)\n",
    "# fill up the memory with enough samples to\n",
    "# take one batch for optimization of the network\n",
    "memory, stacked_frames = fill_up_memory(memory_size, batch_size)\n",
    "\n",
    "# The global step is used to adjust the epsilon value\n",
    "# This is input for the decay step in the predict action\n",
    "# It will lower our exploration rate as it increases\n",
    "global_step=0\n",
    "\n",
    "# create the directory to save the models\n",
    "# if the directory does not exist yet\n",
    "if not os.path.exists(f\"./models/model_{run_id}/\"):\n",
    "    os.makedirs(f\"./models/model_{run_id}\")\n",
    "\n",
    "for episode in range(episodes+1):\n",
    "    # Choose what text file you want to save the progress of your run in.\n",
    "    with open(f\"{run_id}.txt\",\"a+\") as f:\n",
    "        game.new_episode()  # Starts a new episode\n",
    "        state = game.get_state().screen_buffer  # Gets the initial state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  # Initial state\n",
    "        total_reward = 0\n",
    "        done = game.is_episode_finished()\n",
    "        episode_step = 0  # Reset global_step at the start of each episode if it's episode-wise\n",
    "\n",
    "        while episode_step<max_steps and not done:\n",
    "            episode_step += 1  # Increment total steps for the episode\n",
    "            global_step += 1  # Increment decay step\n",
    "\n",
    "            # Select action based on the current state and epsilon\n",
    "            action, explore_probability = predict_action(epsilon_start, epsilon_end, epsilon_decay, global_step, state, possible_actions_pytorch, dqn_network, device)\n",
    "\n",
    "            # Execute action\n",
    "            reward = game.make_action(possible_actions_vizdoom[action]) # do a move based on the action from predict_action\n",
    "            done = game.is_episode_finished()  # Check if the episode is finished\n",
    "            if not done:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "            else:\n",
    "                next_state = np.zeros((84,84), dtype=np.int_)  # Use a zero array for the final state\n",
    "\n",
    "            # Stack the frames of the next state\n",
    "            # This also processes the frames\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "            # Accumulate total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # Save experience to memory\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            if len(memory.buffer) > batch_size:\n",
    "\n",
    "                experiences, buffer_size = memory.sample(batch_size)\n",
    "                loss = optimize_model(experiences, dqn_network, optimizer, gamma, device)\n",
    "\n",
    "            state = next_state  # Update state\n",
    "            del experiences, next_state # delete everything to make sure memory usage is optimized\n",
    "\n",
    "            # Logging\n",
    "        writer.add_scalar('Total Reward/Episode', total_reward, episode)\n",
    "        writer.add_scalar('Loss/Episode', loss, episode)\n",
    "        log_info = f\"Episode: {episode}\\tTotal Reward: {total_reward}\\tExplore P: {explore_probability:.4f}\\tTraining Loss: {loss:.4f}\\tBuffer size: {buffer_size:.4f}\\n\"\n",
    "        print(log_info.strip())  # Print episode summary\n",
    "        f.write(log_info)  # Write episode summary to file\n",
    "        if episode % 50 == 0:\n",
    "            torch.save(dqn_network.state_dict(), \"./models/model_{}/episode_{}.pth\".format(run_id,episode))\n",
    "            print(\"Model Saved\")\n",
    "\n",
    "# Close the writer after training\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1714474795663,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "Q_nHOU2dBYfL"
   },
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEk0ZE2oBYfL"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsV2RDm-BYfL"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ4.b (6 punten)\n",
    "Probeer nu de prestaties van het model te verbeteren en voer bovenstaande stappen nog eens uit. Dit kan je ondertussen op heel veel manieren doen, je kan bijvoorbeeld veranderingen maken aan de hyperparameters, exploratie regel, replay etc.\n",
    "\n",
    "* Leg uit waarom je verwachtte dat jouw aanpassing het beter zou maken\n",
    "* Laat zien of het werkte op basis van traingsgrafiek (stap 1 hierboven)\n",
    "* Kijk goed naar het gedrag na 500 episodes leren of je daar iets van kan afleiden (stap 2 hierboven).\n",
    "\n",
    "Het aantal trainings episodes is technische gezien een hyperparameter, maar die bedoelen we dus niet :)\n",
    "\n",
    "**Let op:**\n",
    "1. Elke keer voordat je een nieuwe training uitvoert met nieuwe hyperparameters moet je de game opnieuw initialseren: <br>\n",
    "``\n",
    "game, possible_actions = create_environment()\n",
    "`` <br>\n",
    "\n",
    "Dan is het geheugen van de CoLab instantie (RAM) ook weer leeg en het netwerk klaar om te gaan leren. Ook nu gaat het om je idee, niet meteen of het netwerk ook beter leert, maar dat is natuurlijk wel een mooie uitkomst!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4CQfy-ABYfM"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Httx9WBYfM"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTWebYD0BYfM"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì__Q5 (2 Punten)__\n",
    "Bestudeer onderstaande code, hoe verschilt de decision policy van de gedrags simulatie met die van de training agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-Xpg1ymBYfM"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1938,
     "status": "ok",
     "timestamp": 1714146752704,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "ZXfsRTv7BYfN",
    "outputId": "6ff904a2-4094-4c16-d7f2-c193744ffb81"
   },
   "outputs": [],
   "source": [
    "# Assuming your DQNetwork and other required classes are already defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize environment\n",
    "env = create_environment()\n",
    "game, possible_actions_vizdoom, possible_actions_pytorch=env\n",
    "state_size = [4, 84, 84]\n",
    "\n",
    "# define which run and which episode you want to load\n",
    "run_id=run_id\n",
    "episode=150\n",
    "\n",
    "# Load the model\n",
    "model_path = f\"./models/model_{run_id}/episode_{episode}.pth\"\n",
    "model = DQNetwork(state_size, len(possible_actions_pytorch)).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "game.init()\n",
    "\n",
    "trajectories={}\n",
    "num_samples=4\n",
    "\n",
    "for i in range(num_samples):  # If you want to run more than one episode, change the range\n",
    "    total_score = 0\n",
    "    trajectory = []\n",
    "    stacked_frames=[]\n",
    "    done = False\n",
    "    game.new_episode()\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "        # Prepare state for the model\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # Get Q values for each action and choose the action with the highest Q\n",
    "        Qs = model(state_tensor)\n",
    "        choice = torch.argmax(Qs).item()\n",
    "        action = possible_actions_vizdoom[choice]\n",
    "\n",
    "        # Execute the action and get the reward\n",
    "        game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            trajectory.append(next_state)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "    score = game.get_total_reward()\n",
    "    print(f\"Score game {i}: \", score)\n",
    "    trajectories[f\"game_{i}\"]=trajectory\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1658,
     "status": "ok",
     "timestamp": 1714146858064,
     "user": {
      "displayName": "toon van Gelderen",
      "userId": "00108286247783493084"
     },
     "user_tz": -120
    },
    "id": "ICohocOhBYfN",
    "outputId": "95cd23dc-2533-4099-cbda-c6261857109f"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "if not os.path.exists(f'./videos/{run_id}/'):\n",
    "    os.makedirs(f'./videos/{run_id}/')\n",
    "\n",
    "# Save to GIF\n",
    "for game in range(num_samples):\n",
    "    imageio.mimsave(f'./videos/{run_id}/video_{game}.gif', trajectories[f\"game_{game}\"], fps=15)\n",
    "    print(f'trajectory game_{game} saved as ./videos/{run_id}/video_{game}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiATVLJiBYfN"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Smart Exploration & Beyond..\n",
    "Je hebt hier gezien dat we weer e-greedy gebruikte om met de explore exploit trade-off om te gaan. Maar we hadden eerder ook al wel gezien dat dit niet altijd de slimste of beste manier van exploratie is. We hebben UCB besproken die niet bezochte states een exploratie bonus geeft, en die bonus neemt af naarmate er meer over een staat geleerd is. Zo kom je niet meer terug bij \"slechte\" states.\n",
    "\n",
    "Nu willen we niet alle mogelijk states per se bezoeken, van sommige kunnen we misschien al direct voorspellen dat deze niets gaan opleveren. In deze context is Curiosity Based Learning dus heel interessant. Kijk nogmaals naar [Curiosity driven learning](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359) voor een goede uitleg.\n",
    "\n",
    "Het gaat nu te ver om hier dieper op in te gaan maar ik hoop dat door de basis te snappen van DQNs en dat dit je nu al een beter idee hebt hoe deze algoritmes werken. Probeer ook zeker zelf verschillende games van Vizdoom uit, of andere games zoals Space invaders. Er zijn eindeloos veel tutorials te vinden op het internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Xvcta0bu98P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
