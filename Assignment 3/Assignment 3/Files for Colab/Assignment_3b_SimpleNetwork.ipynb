{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3b Cognitive Modelling\n",
    "\n",
    "Dit is onderdeel van de derde opdracht voor Cognitive Modelling. Hieronder volgen vragen over 1-layer Q Networks. In totaal bestaat de derde opdracht uit drie onderdelen:\n",
    "* Assignment 3a\n",
    "* __Assignment 3b__ (SimpleNetwork, in Google Colab) \n",
    "* Assignment 3c (Doom, in Google Colab)\n",
    "\n",
    "Bij elke vraag staat de hoeveelheid punten die je er voor kan krijgen. In totaal kan je voor deze opdracht __19 punten__ halen. Geef antwoorden in blokken met code of met tekst. Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hier om wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig. \n",
    "\n",
    "Sla het uiteindelijke notebook op met jullie studentnummers en achternamen in de filenaam: `studentnummer1_achternaam1_studentnummer2_achternaam2_opdrachtnummer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Q Networks\n",
    "\n",
    "We hebben nu gewerkt met een kleine wereld met maar maximaal 64 states. De echte wereld, en veel leuke spelletjes, hebben natuurlijk een veel grotere state space, en dan wordt het al snel erg lastig om nog een Q table te gaan bijhouden. Dit is waar Q netwerken heel handig zijn, en dan met name Deep Q Networks (DQN). Een deep neural network kan helpen de state space een stuk beter generaliseerbaar te maken, en vergelijkbare Q-values toekennen aan states die veel op elkaar lijken, wat een hele waardevolle eigenschap blijkt (zie bijv. de oorspronkelijke DQN paper over Atari games leren spelen [hier](https://arxiv.org/pdf/1312.5602v1.pdf)).\n",
    "\n",
    "\n",
    "__TensorFlow__\n",
    "\n",
    "In deze opdracht gaan we nu geen deep Q netwerk in elkaar zetten maar wel een simpel 1-layer Q netwerk, om te leren hoe deze werken en hoe het Q-learning algoritme kan worden geÃ¯mplementeerd in een netwerk. We maken hier gebruik van pytorch, een library waarmee je (deep) neural networks kan definiÃ«ren en trainen.\n",
    "\n",
    "__FrozenLake__\n",
    "\n",
    "We gebruiken voor deze eerste uitleg weer de kleine frozenlake omgeving: `gym.make('FrozenLake-v1',map_name=\"4x4\",is_slippery=False, render_mode=\"rgb_array\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# init Lake\n",
    "env_4x4 = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"4x4\",\n",
    "    is_slippery=False,\n",
    "    render_mode=\"rgb_array\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We gaan een simpel Q netwerk opzetten waarbij er voor elke state (in totaal 16) een node is in de input laag, en voor elke actie (4) een node in the output laag. \n",
    "\n",
    "Het doel is om de gewichten (weights) van het netwerk zo te trainen dat een gegeven state als input correspondeert met de 4 output Q-values van de 4 verschillende acties voor die state.\n",
    "\n",
    "![](Images\\QNET.png)\n",
    "##### _in deze illustratie zie je het netwerk twee keer in verschillende states_\n",
    "\n",
    "\n",
    "Vervolgens kunnen we een *argmax* gebruiken om de actie met met hoogste Q-value te selecteren. Er zijn 16 input nodes, waarbij elke state nu gerepresenteerd kan worden met een ðŸ”¥[one-hot encoding](https://en.wikipedia.org/wiki/One-hot)ðŸ”¥, dus door alleen de input node van die state op 1 te zetten en alle andere input nodes op 0.\n",
    "\n",
    "Het 'Qnetwork' is gedefineerd als de matrix vermenigvuldiging van inputs en weights, dus simpelweg de som van de inputs vermenigvuldig met de weights voor elke output (deze manier om een neural network te definiÃ«ren zou je bekend moeten voorkomen uit **Leren**). Er is hier dus *geen* activatiefunctie (zoals bijv. Sigmoid of ReLU), i.e. dit is een netwerk met alleen een lineaire layer.\n",
    "\n",
    "__Prediction error & learning__\n",
    "\n",
    "Heel erg vergelijkbaar met Q-learning zoals je het hiervoor gezien hebt is er een prediction error:\n",
    "\n",
    "$$\\delta = r_{t+1} + \\gamma\\ max_a\\ Q(s_{t+1} , a) âˆ’ Q(s_t , a_t)$$\n",
    "\n",
    "We gaan er dus weer even van uit dat in de volgene state $s_{t+1}$ de actie $a$ met de hoogste Q-value wordt gekozen (we hebben in de colleges gezien dat andere opties ook mogelijk zijn). \n",
    "\n",
    "Alleen nu is het niet zo dat de Q-values direct worden ge-update met de de prediction error $\\delta$ maar dat de gewichten van de verbindingen in het netwerk worden aangepast. Deze worden zo aangepast om de toekomstige prediction error te minimaliseren. \n",
    "\n",
    "Het gaat voor deze opdracht te ver om in te gaan op hoe de weights in het netwerk worden aangepast, maar als het goed is leer je dit bij een ander vak. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for Q-learning\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(state_size, action_size,bias=False)\n",
    "        torch.nn.init.uniform_(self.fc.weight, 0, )  # Uniform initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def q_network(env, num_episodes=250, num_rounds=999,PATH=None,filename=None):\n",
    "    state_size = env.observation_space.n\n",
    "    print(env.observation_space)\n",
    "    action_size = env.action_space.n\n",
    "    print(env.action_space)\n",
    "    model = QNetwork(state_size, action_size)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if PATH != None and (not os.path.exists(PATH)):\n",
    "        os.makedirs(f\"{PATH}\")\n",
    "\n",
    "    # Set learning parameters\n",
    "    y = 0.97\n",
    "    e = 0.3\n",
    "\n",
    "    # Create lists to contain total rewards and steps per episode\n",
    "    jList = [] # list of steps taken in episode\n",
    "    rList = [] # list of rewards\n",
    "    aList = [] # list of actions\n",
    "    Qvalues_init = [] # list of Qvalues per state begin\n",
    "    Qvalues_final = [] # list of Qvalues per state end\n",
    "\n",
    "    W_init = model.fc.weight.data.clone()  # Save initial weights\n",
    "\n",
    "    # Get Q_values\n",
    "    for state in range(state_size):\n",
    "        state_tensor = torch.FloatTensor(np.identity(state_size)[state]).unsqueeze(0)\n",
    "        Q_values = model(state_tensor)\n",
    "        Qvalues_init.append(Q_values.detach().numpy())\n",
    "\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        rAll = 0\n",
    "        done = False\n",
    "        j = 0\n",
    "        while not done and j < num_rounds:\n",
    "            j += 1\n",
    "\n",
    "            # make one hot encoding state+get Q_values\n",
    "            state_tensor = torch.FloatTensor(np.identity(state_size)[state]).unsqueeze(0)\n",
    "            Q_values = model(state_tensor)\n",
    "\n",
    "            # choose an action based on the Q_values for that state\n",
    "            _, predicted_action = torch.max(Q_values, 1)\n",
    "\n",
    "            #e greedy:\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = predicted_action.item()\n",
    "            aList.append(action)\n",
    "\n",
    "            #Get new state and reward from environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # get the next state and reward for action\n",
    "            if done and reward == 0:\n",
    "                reward = -1\n",
    "\n",
    "            # Obtain the Q values of the next state by feeding the new state through our network, \n",
    "            # and again assuming you will choose the action with the highest Q value\n",
    "            next_state_tensor = torch.FloatTensor(np.identity(state_size)[next_state]).unsqueeze(0)\n",
    "            next_Q_values = model(next_state_tensor)\n",
    "\n",
    "            # Obtain maxQ' of next state and use this to update the Q value of our chosen action.\n",
    "            targetQ = Q_values.detach().clone()\n",
    "            targetQ[0][action] = reward + y * torch.max(next_Q_values)#in list of targetQ values for s, update the Q value of chosen action\n",
    "\n",
    "            # targetQ, where you only updated the Q-value of the chosen action, is the set of Q-values that you want to network to \n",
    "            # output next time the agent is in state s. In a network we cannot directly update Qvalues because they are the output of the weights \n",
    "            # so we will tune the weights such that they will result in something closer to targetQ. \n",
    "            # The prediction error is thus the difference between targetQ and Qout\n",
    "            \n",
    "            # Train our network using targetQ list, we try to adjust weights in order to \n",
    "            # minimize prediction error or squared error. \n",
    "            loss = criterion(Q_values, targetQ)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            rAll += reward\n",
    "            state = next_state\n",
    "\n",
    "        e*=0.995\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)        \n",
    "        \n",
    "    Weights = model.fc.weight.data.clone()  # Save weights after training\n",
    "\n",
    "    # Get Q_values for final state\n",
    "    for state in range(state_size):\n",
    "        state_tensor = torch.FloatTensor(np.identity(state_size)[state]).unsqueeze(0)\n",
    "        Q_values = model(state_tensor)\n",
    "        Qvalues_final.append(Q_values.detach().numpy())\n",
    "\n",
    "    # save the model\n",
    "    if PATH != None:\n",
    "        if filename==None:\n",
    "            filename=\"model\"\n",
    "        torch.save(model.state_dict(), PATH+filename+\".pt\")\n",
    "        print(\"model saved as \", PATH+filename+\".pt\")\n",
    "\n",
    "    return (rList, jList, action, W_init, Weights, Qvalues_init, Qvalues_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### __Q1.a__ (4 punten)\n",
    "Bestudeer bovenstaande code. Er zijn een aantal verschillen en overeenkomsten met standaard Q-learning met Q tables. Beantwoord de volgende vragen:\n",
    "\n",
    "1. Maakt dit algoritme een explore-exploit trade-off? Zo ja hoe?\n",
    "2. Is er een equivalent van de learning rate? \n",
    "3. Wat is de prediction error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### __Q1.b__ (4 punten)\n",
    "We gaan het netwerk nu 1 keuze laten maken en goed kijken wat er precies gebeurt als we het onderstaande code blok runnen. Kijk goed naar de weights vÃ³Ã³r en na de eerste move. Wat verandert er? Hoe kan dit? En wat laat dit voor verschil zien met Q-nets vs Q-tables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=q_network(env_4x4, num_episodes=1, num_rounds=1)\n",
    "\n",
    "print(\"Inital weights:\")\n",
    "print(res[5][0][0])\n",
    "\n",
    "print(\"\\nreward:\", res[0][0])\n",
    "print(\"chosen action:\", res[2], \"\\n\")\n",
    "\n",
    "print(\"Weights after training\")\n",
    "print(res[6][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We trainen nu nogmaals het netwerk maar nu met 500 episodes en 99 rounds per episode. We maken ook even twee grafiekjes om te zien hoe goed het model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"models/FrozenLake/\"\n",
    "res=q_network(env_4x4, num_episodes=500,PATH=model_dir)\n",
    "width = 20\n",
    "\n",
    "print('Rewards')\n",
    "res1 = np.asarray(res[0])\n",
    "result1 = res1[:(res1.size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "plt.plot(result1)\n",
    "plt.show()\n",
    "\n",
    "print('Steps')\n",
    "res2 = np.asarray(res[1])\n",
    "result2 = res2[:(res2.size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "plt.plot(result2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "We zien dat het model heel snel leert wat de korste weg is. Natuurlijk gaat het nog niet altijd goed, ook omdat er een greedy beslis regel is zal er al af en toe een verkeerde keuze worden gemaakt. \n",
    "\n",
    "<br>\n",
    "\n",
    "### __Q1.c__ (3 punten)\n",
    "Laat nu ook zien wat de weights/Q-values zijn voor de vier mogelijke acties in het hokje links naast de frisbee (state 14). Heeft de beste actie ook de hoogste waarde?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# laat je netwerk spelen\n",
    "\n",
    "Nu je je netwerk hebt getraind kan je hem natuurlijk ook het frozen lake laten oplossen. Met de volgende code kan je je netwerk de gym laten spelen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImage(image):\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=env_4x4\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "model = QNetwork(state_size, action_size)\n",
    "num_rounds = 10\n",
    "num_episodes= 2\n",
    "\n",
    "PATH=model_dir\n",
    "filename=\"model\"\n",
    "\n",
    "model.load_state_dict(torch.load(PATH+filename+\".pt\"))\n",
    "model.eval()\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "\n",
    "    while not done and j < num_rounds:\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        state_tensor = torch.FloatTensor(np.identity(state_size)[state]).unsqueeze(0)\n",
    "\n",
    "        Q_values = model(state_tensor)\n",
    "        _, predicted_action = torch.max(Q_values, 1)\n",
    "\n",
    "        action = predicted_action.item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if done and reward == 0:\n",
    "            reward = -1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        displayImage(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working Memory --> Experience Replay\n",
    "\n",
    "Over de tijd zijn de Q_networks op vele manier uitgebreid om nog beter te kunnen leren. Een ding waar we straks naar gaan kijken is het toevoegen van meerdere lagen. Maar zo is er ook het idee van experience replay, iets wat gebaseerd is op hoe de hersens van mensen en dieren werken  (zie colleges).\n",
    "\n",
    "Experience is dat het netwerk, tussen leerepisodes door, zich herinnert wat hij hiervoor gedaan heeft en wat de uitkomst daar van was. Deze herinneringen worden dan weer gebruikt om van te leren (alsof elke herinnering een echte gebeurtenis was). Experience replay wordt veel gebruikt in deep reinforcement learning, en helpt daar het netwerk stabiele representaties vormen. \n",
    "\n",
    "Wat we hier gaan doen is experience replay toevoegen aan dit simpele Q_netwerk. Het zal dit netwerk niet meteen veel beter maken, omdat het probleem erg makkelijk is en het netwerk simpel, maar kan wel overzichtelijk het principe illustreren. \n",
    "\n",
    "Je hebt hier eigenlijk maar een aantal dingen voor nodig; een memory buffer \n",
    " \n",
    " `memory = []`\n",
    "\n",
    "In deze buffer sla voor elke ronde op wat er gebeurde. De staat waar in je was, welke actie je hebt ondernomen, of je een beloning kreeg, welke staat je terecht kwam en of dit het einde van de episode was):   \n",
    "\n",
    "`memory.append((state, action, reward, next_state, done))`\n",
    "\n",
    "Dan, aan het eind van elke episode, haal je 30 willekeurige herinneringen boven en speelt deze weer uit alsof het echt gebeurde (dus zorgt voor een zelfde update in weights als normaal leren). Bij het begin van een nieuwe leer episode wordt de buffer weer leeggemaakt om ruimte te maken voor nieuwe evaringen. \n",
    "``` python\n",
    "if replay > 0:\n",
    "    for _ in range(30):\n",
    "        state, action, reward, next_state, done = random.choice(memory)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(np.identity(state_size)[state]).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(np.identity(state_size)[next_state]).unsqueeze(0)\n",
    "        \n",
    "        Q_values = model(state_tensor)\n",
    "        next_Q_values = model(next_state_tensor)\n",
    "        \n",
    "        target_Q = Q_values.detach().clone()\n",
    "        target_Q[0][action] = reward + y * torch.max(next_Q_values) * (not done)\n",
    "        \n",
    "        loss = criterion(Q_values, target_Q)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "zorg ervoor dat je in je functie met behulp van de flag `replay` de optie om met replay memory te werken aan en uit kan zetten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Q2.a__ (4 punten)\n",
    "Implementeer deze experience replay, door de bovenstaande code op de juiste plekken in `q_network()` te plakken.  Vergelijk de prestatie van het model met en zonder replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_replay=q_network(env, num_episodes=500, replay=1)\n",
    "width = 20\n",
    "\n",
    "print('Rewards')\n",
    "res1 = np.asarray(res_replay[0])\n",
    "result1 = res1[:(res1.size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "plt.plot(result1)\n",
    "plt.show()\n",
    "\n",
    "print('Steps')\n",
    "res2 = np.asarray(res_replay[1])\n",
    "result2 = res2[:(res2.size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "plt.plot(result2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Q2.b__ (4 punten)\n",
    "\n",
    "We hebben nu naar replay gekeken waarbij je naar willekeurige herinneringen kijkt van de laatste 30 rondes. Het zal waarschijnlijk in deze context niet veel hebben opgeleverd. Ook is dit misschien niet de beste manier van herinneringen ophalen. \n",
    "\n",
    "\n",
    "Er zijn verschillende algoritmes bedacht waarbij niet naar willekeurige, maar juist naar specifieke herinneringen werd gekeken om het leren nog verder te optimaliseren. Bedenk een optimalisatie van de replay functie, en beschrijf hoe je dat zou implementeren. Schrijf in je antwoord je motivatie voor je aanpassing (die is belangrijker dan het slagen er van, natuurlijk kan je als je wilt laten zien dat je netwerk beter of slechter werkt maar dat hoeft niet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
